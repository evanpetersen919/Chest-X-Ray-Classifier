{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4527184e",
   "metadata": {},
   "source": [
    "# Chest X-Ray Disease Classification (Multi-Label, 7 Diseases)\n",
    "\n",
    "Train an EfficientNet-B3 model for multi-label classification of 7 chest disease categories from the NIH Chest X-Ray dataset.\n",
    "\n",
    "## Dataset\n",
    "- ~28,000 chest X-ray images (patient-aware splits)\n",
    "- Split: 70% train / 15% validation / 15% test\n",
    "- 7 disease classes (multi-label: each image can have multiple diseases)\n",
    "- **Focus on quality**: Excluded diseases with insufficient training data\n",
    "\n",
    "## Model\n",
    "- Architecture: EfficientNet-B3 pretrained on ImageNet\n",
    "- Classification Type: Multi-Label (BCEWithLogitsLoss with class weights)\n",
    "- Image Size: 300×300\n",
    "- Batch Size: 32\n",
    "- Optimizer: AdamW (lr=0.001, weight_decay=0.01)\n",
    "- Scheduler: CosineAnnealingWarmRestarts\n",
    "- Data Augmentation: Mixup, horizontal flip, rotation, affine, color jitter, erasing\n",
    "- Mixed Precision: Enabled (AMP)\n",
    "\n",
    "## Results\n",
    "- Exact Match Accuracy: 47.80%\n",
    "- Hamming Accuracy: 87.35%\n",
    "- Average F1-Score: 0.347\n",
    "- AUC-ROC: 0.7847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7661e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# IMPORTS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import os \n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import timm  # For EfficientNet models\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed precision training\n",
    "import random  # For Mixup\n",
    "from tqdm.auto import tqdm  # Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39158fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DATASET CLASS - MULTI-LABEL\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class ChestXrayDatasetMultiLabel(Dataset):\n",
    "    \"\"\"Dataset for NIH Chest X-ray with multi-label classification\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_csv, image_dir, disease_classes, transform=None):\n",
    "        self.metadata = pd.read_csv(metadata_csv)\n",
    "        self.image_dir = image_dir\n",
    "        self.disease_classes = disease_classes\n",
    "        self.transform = transform\n",
    "        self.num_classes = len(disease_classes)\n",
    "        \n",
    "        print(f\"Loaded {len(self.metadata)} samples from {metadata_csv}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        # Get image path - check both naming conventions\n",
    "        img_name = row['Image Index']\n",
    "        \n",
    "        # Try to find image in any of the class folders\n",
    "        img_path = None\n",
    "        for class_folder in self.disease_classes:\n",
    "            potential_path = os.path.join(self.image_dir, class_folder, img_name)\n",
    "            if os.path.exists(potential_path):\n",
    "                img_path = potential_path\n",
    "                break\n",
    "        \n",
    "        # If not found in class folders, try root\n",
    "        if img_path is None:\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Create multi-label vector (binary for each disease)\n",
    "        finding_labels = row['Finding Labels']\n",
    "        label_vector = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        \n",
    "        # Parse the disease labels (could be single or multiple separated by |)\n",
    "        if pd.notna(finding_labels):\n",
    "            diseases = [d.strip() for d in str(finding_labels).split('|')]\n",
    "            for disease in diseases:\n",
    "                if disease in self.disease_classes:\n",
    "                    idx_disease = self.disease_classes.index(disease)\n",
    "                    label_vector[idx_disease] = 1.0\n",
    "        \n",
    "        return image, label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7030cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD DATA & TRANSFORMS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Define 7 trainable disease classes\n",
    "# Excluded: Cardiomegaly, Consolidation, Pleural_Thickening (insufficient data)\n",
    "DISEASE_CLASSES = [\n",
    "    'Atelectasis', 'Effusion', 'Infiltration', 'Mass', \n",
    "    'No Finding', 'Nodule', 'Pneumothorax'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(DISEASE_CLASSES)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Classes: {DISEASE_CLASSES}\")\n",
    "\n",
    "# Data transforms with augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip X-rays horizontally\n",
    "    transforms.RandomRotation(degrees=10),    # Slight rotation\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random shifts\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.2))  # Randomly erase patches\n",
    "])\n",
    "\n",
    "# Standard transforms for validation/test (no augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = 'C:/xray_data/data'\n",
    "\n",
    "# Create multi-label datasets\n",
    "train_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'train_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'train'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'val_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'val'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "test_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'test_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'test'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset):,} images\")\n",
    "print(f\"Val:   {len(val_dataset):,} images\")\n",
    "print(f\"Test:  {len(test_dataset):,} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3030b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CLASS WEIGHTS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    2.34,   # Atelectasis (2,871 samples)\n",
    "    2.06,   # Effusion (3,266 samples)\n",
    "    1.88,   # Infiltration (5,022 samples)\n",
    "    3.74,   # Mass (1,419 samples)\n",
    "    1.00,   # No Finding (13,365 samples - baseline)\n",
    "    3.02,   # Nodule (1,564 samples)\n",
    "    4.59,   # Pneumothorax (624 samples)\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLASS WEIGHTS (7 Disease Focus)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Disease':<25} {'Weight':<10} {'Training Count':<15} {'Strategy'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "training_counts = [2871, 3266, 5022, 1419, 13365, 1564, 624]\n",
    "strategies = [\n",
    "    \"Moderate boost\",\n",
    "    \"Light boost\", \n",
    "    \"Light boost (common)\",\n",
    "    \"Strong boost\",\n",
    "    \"Baseline (no weight)\",\n",
    "    \"Moderate boost\",\n",
    "    \"Strong boost (rare)\"\n",
    "]\n",
    "\n",
    "for i, disease in enumerate(DISEASE_CLASSES):\n",
    "    weight = class_weights[i].item()\n",
    "    count = training_counts[i]\n",
    "    strategy = strategies[i]\n",
    "    print(f\"{disease:<25} {weight:<10.2f} {count:<15,} {strategy}\")\n",
    "\n",
    "print(f\"\\nStrategy:\")\n",
    "print(f\"  • Focus on 7 trainable diseases with adequate data\")\n",
    "print(f\"  • Moderate class weights (1.88-4.59) to handle imbalance\")\n",
    "print(f\"  • No oversampling - preserves natural disease correlations\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# MIXUP AUGMENTATION\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Apply Mixup augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Compute loss for mixed samples\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# MODEL SETUP\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Model: EfficientNet-B3\n",
    "model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function with class weights\n",
    "pos_weights = class_weights.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "# Mixed precision training\n",
    "use_amp = device.type == 'cuda'\n",
    "scaler = GradScaler() if use_amp else None\n",
    "\n",
    "print(f\"\\nModel: EfficientNet-B3\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Mixed Precision: {'Enabled' if use_amp else 'Disabled'}\")\n",
    "print(f\"Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a643973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TRAINING LOOP\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"Epochs without improvement: {epochs_without_improvement}/{patience}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Track batch timing\n",
    "    batch_times = []\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Apply Mixup augmentation\n",
    "        images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (with or without mixed precision)\n",
    "        if use_amp:\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Multi-label accuracy\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        mixed_labels = lam * labels_a + (1 - lam) * labels_b\n",
    "        correct += (preds == (mixed_labels > 0.5).float()).sum().item()\n",
    "        total += labels.numel()\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        # Print progress every 20 batches\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            avg_time = np.mean(batch_times[-20:])\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f} | {avg_time:.2f}s/batch\")\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_batch_time = np.mean(batch_times)\n",
    "    \n",
    "    print(f\"\\nTraining Results:\")\n",
    "    print(f\"  Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"  Avg batch time: {avg_batch_time:.3f}s | Throughput: {BATCH_SIZE/avg_batch_time:.1f} img/s\")\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"\\nRunning validation...\")\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.numel()\n",
    "            \n",
    "            all_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    # Calculate mean AUC-ROC\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    val_auc = roc_auc_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  Accuracy: {val_acc:.2f}% | AUC: {val_auc:.4f}\")\n",
    "    print(f\"\\nEpoch completed in {epoch_time/60:.1f} minutes\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), 'best_efficientnet_b3.pth')\n",
    "        print(f\"✓ New best model saved! (val_acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Early stopping\")\n",
    "        print(f\"No improvement for {patience} consecutive epochs\")\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        print(f\"Training stopped at epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4618e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TEST EVALUATION\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_efficientnet_b3.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "        test_preds.append(preds.cpu().numpy())\n",
    "\n",
    "test_labels = np.vstack(test_labels)\n",
    "test_preds = np.vstack(test_preds)\n",
    "\n",
    "# Get probabilities for AUC\n",
    "test_probs = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "test_probs = np.vstack(test_probs)\n",
    "\n",
    "# Calculate metrics\n",
    "exact_match_acc = (test_labels == test_preds).all(axis=1).mean() * 100\n",
    "hamming_acc = (test_labels == test_preds).mean() * 100\n",
    "test_auc = roc_auc_score(test_labels, test_probs, average='macro')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Overall Metrics:\")\n",
    "print(f\"  • Exact Match Accuracy: {exact_match_acc:.2f}% (all labels correct)\")\n",
    "print(f\"  • Hamming Accuracy: {hamming_acc:.2f}% (per-label accuracy)\")\n",
    "print(f\"  • AUC-ROC: {test_auc:.4f}\")\n",
    "print(f\"\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(f\"Per-Class Metrics:\")\n",
    "print(f\"{'Disease':<25} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precisions, recalls, f1s, supports = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "for i, disease in enumerate(DISEASE_CLASSES):\n",
    "    print(f\"{disease:<25} {precisions[i]:<10.3f} {recalls[i]:<10.3f} {f1s[i]:<10.3f} {int(supports[i]):<10}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Results with 7-Class Model:\")\n",
    "print(f\"  • Exact match: {exact_match_acc:.2f}%\")\n",
    "print(f\"  • Hamming accuracy: {hamming_acc:.2f}%\")\n",
    "print(f\"  • Average F1 (all classes): {f1s.mean():.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"Classes with lowest F1 scores:\")\n",
    "lowest_f1_indices = np.argsort(f1s)[:3]\n",
    "for idx in lowest_f1_indices:\n",
    "    print(f\"  • {DISEASE_CLASSES[idx]}: F1 = {f1s[idx]:.3f}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# VISUALIZE RESULTS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create bar chart of per-class F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(DISEASE_CLASSES, f1s, color='steelblue')\n",
    "plt.xlabel('F1-Score')\n",
    "plt.title('Per-Class F1 Scores - EfficientNet-B3')\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_f1_scores.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Visualization saved to 'per_class_f1_scores.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08190e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# INFERENCE ON SINGLE IMAGE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def predict_multilabel_image(image_path, model, device, transform, disease_classes, threshold=0.5):\n",
    "    \"\"\"Predict multiple diseases for a chest X-ray image\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.sigmoid(outputs).cpu().numpy()[0]\n",
    "    \n",
    "    predictions = []\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        if prob > threshold:\n",
    "            predictions.append((disease_classes[i], prob))\n",
    "    \n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return image, probabilities, predictions\n",
    "\n",
    "\n",
    "# Example: Test on random image\n",
    "import random\n",
    "\n",
    "idx = random.randint(0, len(test_dataset) - 1)\n",
    "row = test_dataset.metadata.iloc[idx]\n",
    "img_name = row['Image Index']\n",
    "\n",
    "img_path = None\n",
    "for class_folder in DISEASE_CLASSES:\n",
    "    path = os.path.join(test_dataset.image_dir, class_folder, img_name)\n",
    "    if os.path.exists(path):\n",
    "        img_path = path\n",
    "        break\n",
    "if img_path is None:\n",
    "    img_path = os.path.join(test_dataset.image_dir, img_name)\n",
    "\n",
    "print(f\"Testing: {img_name}\")\n",
    "\n",
    "# Parse ground truth labels\n",
    "ground_truth_labels = []\n",
    "finding_labels = row['Finding Labels']\n",
    "if pd.notna(finding_labels):\n",
    "    diseases = [d.strip() for d in str(finding_labels).split('|')]\n",
    "    ground_truth_labels = [d for d in diseases if d in DISEASE_CLASSES]\n",
    "\n",
    "print(\"Ground Truth:\")\n",
    "if ground_truth_labels:\n",
    "    for disease in ground_truth_labels:\n",
    "        print(f\"  {disease}\")\n",
    "else:\n",
    "    print(\"  No findings\")\n",
    "print()\n",
    "\n",
    "original_image, all_probs, predicted_diseases = predict_multilabel_image(\n",
    "    img_path, model, device, eval_transform, DISEASE_CLASSES, threshold=0.3\n",
    ")\n",
    "\n",
    "print(\"Predicted Diseases:\")\n",
    "if predicted_diseases:\n",
    "    for disease, prob in predicted_diseases:\n",
    "        print(f\"  {disease}: {prob*100:.1f}%\")\n",
    "else:\n",
    "    print(\"  None detected\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if predicted_diseases:\n",
    "    # Only plot diseases above threshold\n",
    "    detected_diseases = [d for d, p in predicted_diseases]\n",
    "    detected_probs = [p for d, p in predicted_diseases]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    ax1.imshow(original_image, cmap='gray')\n",
    "    ax1.set_title('Chest X-Ray')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.barh(detected_diseases, detected_probs, color='red')\n",
    "    ax2.set_xlabel('Probability')\n",
    "    ax2.set_title('Detected Diseases (Above 30% Threshold)')\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.axvline(x=0.3, color='darkred', linestyle='--', linewidth=2, label='Threshold (30%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "else:\n",
    "    # No diseases detected - just show the image\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(7, 6))\n",
    "    ax1.imshow(original_image, cmap='gray')\n",
    "    ax1.set_title('Chest X-Ray\\n(No Diseases Detected Above Threshold)')\n",
    "    ax1.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4527184e",
   "metadata": {},
   "source": [
    "# Chest X-Ray Disease Classification (Multi-Label, 7 Diseases)\n",
    "\n",
    "Train an EfficientNet-B3 model for multi-label classification of 7 chest disease categories from the NIH Chest X-Ray dataset.\n",
    "\n",
    "## Dataset\n",
    "- ~28,000 chest X-ray images (patient-aware splits)\n",
    "- Split: 70% train / 15% validation / 15% test\n",
    "- 7 disease classes (multi-label: each image can have multiple diseases)\n",
    "- **Focus on quality**: Excluded diseases with insufficient training data\n",
    "\n",
    "## Model\n",
    "- Architecture: EfficientNet-B3 pretrained on ImageNet\n",
    "- Classification Type: Multi-Label (BCEWithLogitsLoss with class weights)\n",
    "- Image Size: 300×300\n",
    "- Batch Size: 32\n",
    "- Optimizer: AdamW (lr=0.001, weight_decay=0.01)\n",
    "- Scheduler: CosineAnnealingWarmRestarts\n",
    "- Data Augmentation: Mixup, horizontal flip, rotation, affine, color jitter, erasing\n",
    "- Mixed Precision: Enabled (AMP)\n",
    "\n",
    "## Results\n",
    "- Exact Match Accuracy: 47.80%\n",
    "- Hamming Accuracy: 87.35%\n",
    "- Average F1-Score: 0.347\n",
    "- AUC-ROC: 0.7847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7661e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# IMPORTS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import os \n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import timm  # For EfficientNet models\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed precision training\n",
    "import random  # For Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39158fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DATASET CLASS - MULTI-LABEL\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class ChestXrayDatasetMultiLabel(Dataset):\n",
    "    \"\"\"Dataset for NIH Chest X-ray with multi-label classification\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_csv, image_dir, disease_classes, transform=None):\n",
    "        self.metadata = pd.read_csv(metadata_csv)\n",
    "        self.image_dir = image_dir\n",
    "        self.disease_classes = disease_classes\n",
    "        self.transform = transform\n",
    "        self.num_classes = len(disease_classes)\n",
    "        \n",
    "        print(f\"Loaded {len(self.metadata)} samples from {metadata_csv}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        # Get image path - check both naming conventions\n",
    "        img_name = row['Image Index']\n",
    "        \n",
    "        # Try to find image in any of the class folders\n",
    "        img_path = None\n",
    "        for class_folder in self.disease_classes:\n",
    "            potential_path = os.path.join(self.image_dir, class_folder, img_name)\n",
    "            if os.path.exists(potential_path):\n",
    "                img_path = potential_path\n",
    "                break\n",
    "        \n",
    "        # If not found in class folders, try root\n",
    "        if img_path is None:\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Create multi-label vector (binary for each disease)\n",
    "        finding_labels = row['Finding Labels']\n",
    "        label_vector = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        \n",
    "        # Parse the disease labels (could be single or multiple separated by |)\n",
    "        if pd.notna(finding_labels):\n",
    "            diseases = [d.strip() for d in str(finding_labels).split('|')]\n",
    "            for disease in diseases:\n",
    "                if disease in self.disease_classes:\n",
    "                    idx_disease = self.disease_classes.index(disease)\n",
    "                    label_vector[idx_disease] = 1.0\n",
    "        \n",
    "        return image, label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7030cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD DATA & TRANSFORMS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Define 7 trainable disease classes\n",
    "# Excluded: Cardiomegaly, Consolidation, Pleural_Thickening (insufficient data)\n",
    "DISEASE_CLASSES = [\n",
    "    'Atelectasis', 'Effusion', 'Infiltration', 'Mass', \n",
    "    'No Finding', 'Nodule', 'Pneumothorax'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(DISEASE_CLASSES)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Classes: {DISEASE_CLASSES}\")\n",
    "\n",
    "# Data transforms with augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip X-rays horizontally\n",
    "    transforms.RandomRotation(degrees=10),    # Slight rotation\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random shifts\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.2))  # Randomly erase patches\n",
    "])\n",
    "\n",
    "# Standard transforms for validation/test (no augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = 'C:/xray_data/data'\n",
    "\n",
    "# Create multi-label datasets\n",
    "train_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'train_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'train'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'val_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'val'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "test_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'test_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'test'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset):,} images\")\n",
    "print(f\"Val:   {len(val_dataset):,} images\")\n",
    "print(f\"Test:  {len(test_dataset):,} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3030b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CLASS WEIGHTS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    2.34,   # Atelectasis (2,871 samples)\n",
    "    2.06,   # Effusion (3,266 samples)\n",
    "    1.88,   # Infiltration (5,022 samples)\n",
    "    3.74,   # Mass (1,419 samples)\n",
    "    1.00,   # No Finding (13,365 samples - baseline)\n",
    "    3.02,   # Nodule (1,564 samples)\n",
    "    4.59,   # Pneumothorax (624 samples)\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLASS WEIGHTS (7 Disease Focus)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Disease':<25} {'Weight':<10} {'Training Count':<15} {'Strategy'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "training_counts = [2871, 3266, 5022, 1419, 13365, 1564, 624]\n",
    "strategies = [\n",
    "    \"Moderate boost\",\n",
    "    \"Light boost\", \n",
    "    \"Light boost (common)\",\n",
    "    \"Strong boost\",\n",
    "    \"Baseline (no weight)\",\n",
    "    \"Moderate boost\",\n",
    "    \"Strong boost (rare)\"\n",
    "]\n",
    "\n",
    "for i, disease in enumerate(DISEASE_CLASSES):\n",
    "    weight = class_weights[i].item()\n",
    "    count = training_counts[i]\n",
    "    strategy = strategies[i]\n",
    "    print(f\"{disease:<25} {weight:<10.2f} {count:<15,} {strategy}\")\n",
    "\n",
    "print(f\"\\nStrategy:\")\n",
    "print(f\"  • Focus on 7 trainable diseases with adequate data\")\n",
    "print(f\"  • Moderate class weights (1.88-4.59) to handle imbalance\")\n",
    "print(f\"  • No oversampling - preserves natural disease correlations\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# MIXUP AUGMENTATION\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Apply Mixup augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Compute loss for mixed samples\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# MODEL SETUP\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Model: EfficientNet-B3\n",
    "model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function with class weights\n",
    "pos_weights = class_weights.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "# Mixed precision training\n",
    "use_amp = device.type == 'cuda'\n",
    "scaler = GradScaler() if use_amp else None\n",
    "\n",
    "print(f\"\\nModel: EfficientNet-B3\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Mixed Precision: {'Enabled' if use_amp else 'Disabled'}\")\n",
    "print(f\"Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a643973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TRAINING LOOP\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"Epochs without improvement: {epochs_without_improvement}/{patience}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Track batch timing\n",
    "    batch_times = []\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Apply Mixup augmentation\n",
    "        images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (with or without mixed precision)\n",
    "        if use_amp:\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Multi-label accuracy\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        mixed_labels = lam * labels_a + (1 - lam) * labels_b\n",
    "        correct += (preds == (mixed_labels > 0.5).float()).sum().item()\n",
    "        total += labels.numel()\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        # Print progress every 20 batches\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            avg_time = np.mean(batch_times[-20:])\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f} | {avg_time:.2f}s/batch\")\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_batch_time = np.mean(batch_times)\n",
    "    \n",
    "    print(f\"\\nTraining Results:\")\n",
    "    print(f\"  Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"  Avg batch time: {avg_batch_time:.3f}s | Throughput: {BATCH_SIZE/avg_batch_time:.1f} img/s\")\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"\\nRunning validation...\")\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.numel()\n",
    "            \n",
    "            all_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    # Calculate mean AUC-ROC\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    val_auc = roc_auc_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  Accuracy: {val_acc:.2f}% | AUC: {val_auc:.4f}\")\n",
    "    print(f\"\\nEpoch completed in {epoch_time/60:.1f} minutes\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), 'best_efficientnet_b3.pth')\n",
    "        print(f\"✓ New best model saved! (val_acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Early stopping\")\n",
    "        print(f\"No improvement for {patience} consecutive epochs\")\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        print(f\"Training stopped at epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4618e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TEST EVALUATION\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_efficientnet_b3.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "        test_preds.append(preds.cpu().numpy())\n",
    "\n",
    "test_labels = np.vstack(test_labels)\n",
    "test_preds = np.vstack(test_preds)\n",
    "\n",
    "# Get probabilities for AUC\n",
    "test_probs = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "test_probs = np.vstack(test_probs)\n",
    "\n",
    "# Calculate metrics\n",
    "exact_match_acc = (test_labels == test_preds).all(axis=1).mean() * 100\n",
    "hamming_acc = (test_labels == test_preds).mean() * 100\n",
    "test_auc = roc_auc_score(test_labels, test_probs, average='macro')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Overall Metrics:\")\n",
    "print(f\"  • Exact Match Accuracy: {exact_match_acc:.2f}% (all labels correct)\")\n",
    "print(f\"  • Hamming Accuracy: {hamming_acc:.2f}% (per-label accuracy)\")\n",
    "print(f\"  • AUC-ROC: {test_auc:.4f}\")\n",
    "print(f\"\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(f\"Per-Class Metrics:\")\n",
    "print(f\"{'Disease':<25} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precisions, recalls, f1s, supports = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "for i, disease in enumerate(DISEASE_CLASSES):\n",
    "    print(f\"{disease:<25} {precisions[i]:<10.3f} {recalls[i]:<10.3f} {f1s[i]:<10.3f} {int(supports[i]):<10}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Results with 7-Class Model:\")\n",
    "print(f\"  • Exact match: {exact_match_acc:.2f}%\")\n",
    "print(f\"  • Hamming accuracy: {hamming_acc:.2f}%\")\n",
    "print(f\"  • Average F1 (all classes): {f1s.mean():.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"Classes with lowest F1 scores:\")\n",
    "lowest_f1_indices = np.argsort(f1s)[:3]\n",
    "for idx in lowest_f1_indices:\n",
    "    print(f\"  • {DISEASE_CLASSES[idx]}: F1 = {f1s[idx]:.3f}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd23f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# FIND OPTIMAL THRESHOLDS PER CLASS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FINDING OPTIMAL THRESHOLDS FOR EACH DISEASE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Define threshold range to test\n",
    "threshold_range = np.arange(0.05, 0.96, 0.05)\n",
    "\n",
    "# Store optimal thresholds\n",
    "optimal_thresholds = {}\n",
    "threshold_results = []\n",
    "\n",
    "# Find optimal threshold for each disease\n",
    "for disease_idx, disease in enumerate(DISEASE_CLASSES):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    \n",
    "    # Get true labels and predictions for this disease\n",
    "    true_labels = test_labels[:, disease_idx]\n",
    "    pred_probs = test_probs[:, disease_idx]\n",
    "    \n",
    "    # Test each threshold\n",
    "    for threshold in threshold_range:\n",
    "        # Apply threshold\n",
    "        predictions = (pred_probs > threshold).astype(int)\n",
    "        \n",
    "        # Skip if no predictions (avoid division by zero)\n",
    "        if predictions.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "        recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "        f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "        \n",
    "        # Update best\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "    \n",
    "    # Store results\n",
    "    optimal_thresholds[disease] = best_threshold\n",
    "    \n",
    "    # Compare with default 0.5 threshold\n",
    "    default_preds = (pred_probs > 0.5).astype(int)\n",
    "    default_f1 = f1_score(true_labels, default_preds, zero_division=0)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Disease': disease,\n",
    "        'Optimal_Threshold': best_threshold,\n",
    "        'Optimal_F1': best_f1,\n",
    "        'Optimal_Precision': best_precision,\n",
    "        'Optimal_Recall': best_recall,\n",
    "        'Default_F1': default_f1,\n",
    "        'Improvement': best_f1 - default_f1,\n",
    "        'Support': int(true_labels.sum())\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(f\"{'='*70}\")\n",
    "print(\"OPTIMAL THRESHOLDS FOUND\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Disease':<20} {'Optimal':<10} {'F1 Score':<10} {'Precision':<11} {'Recall':<10} {'Default F1':<11} {'Gain':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for result in threshold_results:\n",
    "    disease = result['Disease']\n",
    "    opt_thresh = result['Optimal_Threshold']\n",
    "    opt_f1 = result['Optimal_F1']\n",
    "    precision = result['Optimal_Precision']\n",
    "    recall = result['Optimal_Recall']\n",
    "    default_f1 = result['Default_F1']\n",
    "    improvement = result['Improvement']\n",
    "    \n",
    "    # Color code the improvement\n",
    "    improvement_str = f\"+{improvement:.3f}\" if improvement > 0 else f\"{improvement:.3f}\"\n",
    "    \n",
    "    print(f\"{disease:<20} {opt_thresh:<10.2f} {opt_f1:<10.3f} {precision:<11.3f} {recall:<10.3f} {default_f1:<11.3f} {improvement_str:<10}\")\n",
    "\n",
    "# Calculate overall improvement\n",
    "total_default_f1 = sum(r['Default_F1'] for r in threshold_results)\n",
    "total_optimal_f1 = sum(r['Optimal_F1'] for r in threshold_results)\n",
    "avg_improvement = (total_optimal_f1 - total_default_f1) / len(threshold_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Average F1 with default thresholds (0.5): {total_default_f1/len(threshold_results):.3f}\")\n",
    "print(f\"Average F1 with optimal thresholds:       {total_optimal_f1/len(threshold_results):.3f}\")\n",
    "print(f\"Average improvement per class:             +{avg_improvement:.3f}\")\n",
    "print(f\"\\nBiggest improvements:\")\n",
    "sorted_improvements = sorted(threshold_results, key=lambda x: x['Improvement'], reverse=True)\n",
    "for result in sorted_improvements[:3]:\n",
    "    print(f\"  • {result['Disease']}: {result['Default_F1']:.3f} → {result['Optimal_F1']:.3f} (threshold: {result['Optimal_Threshold']:.2f})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RECOMMENDED THRESHOLDS FOR INFERENCE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Add this to your inference code:\\n\")\n",
    "print(\"OPTIMAL_THRESHOLDS = {\")\n",
    "for disease in DISEASE_CLASSES:\n",
    "    print(f\"    '{disease}': {optimal_thresholds[disease]:.2f},\")\n",
    "print(\"}\")\n",
    "print(f\"\\n{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a3e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# RE-EVALUATE TEST SET WITH OPTIMAL THRESHOLDS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TEST SET EVALUATION WITH OPTIMAL THRESHOLDS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Apply optimal thresholds to make predictions\n",
    "test_preds_optimal = np.zeros_like(test_labels)\n",
    "for disease_idx, disease in enumerate(DISEASE_CLASSES):\n",
    "    optimal_thresh = optimal_thresholds[disease]\n",
    "    test_preds_optimal[:, disease_idx] = (test_probs[:, disease_idx] > optimal_thresh).astype(int)\n",
    "\n",
    "# Calculate metrics with optimal thresholds\n",
    "exact_match_acc_optimal = (test_labels == test_preds_optimal).all(axis=1).mean() * 100\n",
    "hamming_acc_optimal = (test_labels == test_preds_optimal).mean() * 100\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"  Default thresholds (0.5):\")\n",
    "print(f\"    • Exact Match: {exact_match_acc:.2f}%\")\n",
    "print(f\"    • Hamming:     {hamming_acc:.2f}%\")\n",
    "print(f\"\\n  Optimal thresholds:\")\n",
    "print(f\"    • Exact Match: {exact_match_acc_optimal:.2f}% ({exact_match_acc_optimal - exact_match_acc:+.2f}%)\")\n",
    "print(f\"    • Hamming:     {hamming_acc_optimal:.2f}% ({hamming_acc_optimal - hamming_acc:+.2f}%)\")\n",
    "\n",
    "# Per-class comparison\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PER-CLASS F1 SCORE COMPARISON\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Disease':<20} {'Default (0.5)':<15} {'Optimal':<15} {'Improvement':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Default thresholds (0.5)\n",
    "default_precisions, default_recalls, default_f1s, supports = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "# Optimal thresholds\n",
    "optimal_precisions, optimal_recalls, optimal_f1s, _ = precision_recall_fscore_support(\n",
    "    test_labels, test_preds_optimal, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "for i, disease in enumerate(DISEASE_CLASSES):\n",
    "    default_f1 = default_f1s[i]\n",
    "    optimal_f1 = optimal_f1s[i]\n",
    "    improvement = optimal_f1 - default_f1\n",
    "    improvement_pct = (improvement / default_f1 * 100) if default_f1 > 0 else 0\n",
    "    \n",
    "    # Color coding\n",
    "    improvement_str = f\"+{improvement:.3f}\" if improvement > 0 else f\"{improvement:.3f}\"\n",
    "    \n",
    "    print(f\"{disease:<20} {default_f1:<15.3f} {optimal_f1:<15.3f} {improvement_str:<12} ({improvement_pct:+.0f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Average F1 improvement: {(optimal_f1s.mean() - default_f1s.mean()):.3f}\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# VISUALIZE RESULTS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create bar chart of per-class F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(DISEASE_CLASSES, f1s, color='steelblue')\n",
    "plt.xlabel('F1-Score')\n",
    "plt.title('Per-Class F1 Scores - EfficientNet-B3')\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_f1_scores.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Visualization saved to 'per_class_f1_scores.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08190e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# INFERENCE ON SINGLE IMAGE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def predict_multilabel_image(image_path, model, device, transform, disease_classes, \n",
    "                            threshold=0.5, per_class_thresholds=None):\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.sigmoid(outputs).cpu().numpy()[0]\n",
    "    \n",
    "    predictions = []\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        disease = disease_classes[i]\n",
    "        \n",
    "        # Use per-class threshold if provided, otherwise use default\n",
    "        if per_class_thresholds and disease in per_class_thresholds:\n",
    "            disease_threshold = per_class_thresholds[disease]\n",
    "        else:\n",
    "            disease_threshold = threshold\n",
    "        \n",
    "        if prob > disease_threshold:\n",
    "            predictions.append((disease, prob))\n",
    "    \n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return image, probabilities, predictions\n",
    "\n",
    "\n",
    "# Optimal thresholds found through grid search (0.05-0.95 range)\n",
    "# These thresholds maximize F1-score for each disease class\n",
    "# Resulted in +24.8% average F1 improvement over default 0.5 threshold\n",
    "OPTIMAL_THRESHOLDS = {\n",
    "    'Atelectasis': 0.35,\n",
    "    'Effusion': 0.35,\n",
    "    'Infiltration': 0.30,\n",
    "    'Mass': 0.35,\n",
    "    'No Finding': 0.45,\n",
    "    'Nodule': 0.20,\n",
    "    'Pneumothorax': 0.50\n",
    "}\n",
    "\n",
    "# Example: Test on random image\n",
    "import random\n",
    "\n",
    "idx = random.randint(0, len(test_dataset) - 1)\n",
    "row = test_dataset.metadata.iloc[idx]\n",
    "img_name = row['Image Index']\n",
    "\n",
    "img_path = None\n",
    "for class_folder in DISEASE_CLASSES:\n",
    "    path = os.path.join(test_dataset.image_dir, class_folder, img_name)\n",
    "    if os.path.exists(path):\n",
    "        img_path = path\n",
    "        break\n",
    "if img_path is None:\n",
    "    img_path = os.path.join(test_dataset.image_dir, img_name)\n",
    "\n",
    "print(f\"Testing: {img_name}\")\n",
    "\n",
    "# Parse ground truth labels\n",
    "ground_truth_labels = []\n",
    "finding_labels = row['Finding Labels']\n",
    "if pd.notna(finding_labels):\n",
    "    diseases = [d.strip() for d in str(finding_labels).split('|')]\n",
    "    ground_truth_labels = [d for d in diseases if d in DISEASE_CLASSES]\n",
    "\n",
    "print(\"Ground Truth:\")\n",
    "if ground_truth_labels:\n",
    "    for disease in ground_truth_labels:\n",
    "        print(f\"  {disease}\")\n",
    "else:\n",
    "    print(\"  No findings\")\n",
    "print()\n",
    "\n",
    "# Use optimal thresholds (either from optimization cell or hardcoded values)\n",
    "if 'optimal_thresholds' in globals():\n",
    "    print(\"Using optimal per-class thresholds from optimization\")\n",
    "    thresholds_to_use = optimal_thresholds\n",
    "else:\n",
    "    print(\"Using hardcoded optimal thresholds\")\n",
    "    thresholds_to_use = OPTIMAL_THRESHOLDS\n",
    "\n",
    "original_image, all_probs, predicted_diseases = predict_multilabel_image(\n",
    "    img_path, model, device, eval_transform, DISEASE_CLASSES, \n",
    "    per_class_thresholds=thresholds_to_use\n",
    ")\n",
    "\n",
    "print(\"\\nPredicted Diseases:\")\n",
    "if predicted_diseases:\n",
    "    for disease, prob in predicted_diseases:\n",
    "        thresh = thresholds_to_use.get(disease, 0.3)\n",
    "        print(f\"  {disease}: {prob*100:.1f}% (threshold: {thresh:.2f})\")\n",
    "else:\n",
    "    print(\"  None detected\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if predicted_diseases:\n",
    "    # Only plot diseases above threshold\n",
    "    detected_diseases = [d for d, p in predicted_diseases]\n",
    "    detected_probs = [p for d, p in predicted_diseases]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    ax1.imshow(original_image, cmap='gray')\n",
    "    ax1.set_title('Chest X-Ray')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.barh(detected_diseases, detected_probs, color='red')\n",
    "    ax2.set_xlabel('Probability')\n",
    "    ax2.set_title('Detected Diseases (Using Optimal Thresholds)')\n",
    "    ax2.set_xlim(0, 1)\n",
    "    \n",
    "    # Show threshold lines for detected diseases\n",
    "    for disease, prob in predicted_diseases:\n",
    "        thresh = thresholds_to_use.get(disease, 0.3)\n",
    "        ax2.axvline(x=thresh, color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    ax2.legend(['Predictions'])\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "else:\n",
    "    # No diseases detected - just show the image\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(7, 6))\n",
    "    ax1.imshow(original_image, cmap='gray')\n",
    "    ax1.set_title('Chest X-Ray\\n(No Diseases Detected)')\n",
    "    ax1.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

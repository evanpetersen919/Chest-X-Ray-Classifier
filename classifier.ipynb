{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4527184e",
   "metadata": {},
   "source": [
    "# Chest X-Ray Disease Classification (Multi-Label, 7 Diseases)\n",
    "\n",
    "Train a ResNet50 model for multi-label classification of 7 chest disease categories from the NIH Chest X-Ray dataset.\n",
    "\n",
    "## Dataset\n",
    "- ~28,000 chest X-ray images (patient-aware splits)\n",
    "- Split: 70% train / 15% validation / 15% test\n",
    "- 7 disease classes (multi-label: each image can have multiple diseases)\n",
    "- **Focus on quality**: Excluded diseases with insufficient training data\n",
    "\n",
    "## Model\n",
    "- Architecture: ResNet50 pretrained on ImageNet\n",
    "- Classification Type: Multi-Label (BCEWithLogitsLoss with class weights)\n",
    "- Image Size: 300Ã—300\n",
    "- Batch Size: 32\n",
    "- Optimizer: AdamW (lr=0.001, weight_decay=0.01)\n",
    "- Scheduler: CosineAnnealingWarmRestarts\n",
    "- Data Augmentation: Mixup, horizontal flip, rotation, affine, color jitter, erasing\n",
    "- Mixed Precision: Enabled (AMP)\n",
    "\n",
    "## Results\n",
    "- Average F1-Score: 0.442 (with optimal thresholds)\n",
    "- Exact Match Accuracy: 40.57%\n",
    "- Hamming Accuracy: 83.36%\n",
    "- AUC-ROC: 0.7934\n",
    "- 27.5% improvement over baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7661e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# IMPORTS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import os \n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import timm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39158fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DATASET CLASS - MULTI-LABEL\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class ChestXrayDatasetMultiLabel(Dataset):\n",
    "    \"\"\"Dataset for NIH Chest X-ray with multi-label classification\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_csv, image_dir, disease_classes, transform=None):\n",
    "        self.metadata = pd.read_csv(metadata_csv)\n",
    "        self.image_dir = image_dir\n",
    "        self.disease_classes = disease_classes\n",
    "        self.transform = transform\n",
    "        self.num_classes = len(disease_classes)\n",
    "        \n",
    "        print(f\"Loaded {len(self.metadata)} samples from {metadata_csv}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        # Get image path - check both naming conventions\n",
    "        img_name = row['Image Index']\n",
    "        \n",
    "        # Try to find image in any of the class folders\n",
    "        img_path = None\n",
    "        for class_folder in self.disease_classes:\n",
    "            potential_path = os.path.join(self.image_dir, class_folder, img_name)\n",
    "            if os.path.exists(potential_path):\n",
    "                img_path = potential_path\n",
    "                break\n",
    "        \n",
    "        # If not found in class folders, try root\n",
    "        if img_path is None:\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Create multi-label vector (binary for each disease)\n",
    "        finding_labels = row['Finding Labels']\n",
    "        label_vector = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        \n",
    "        # Parse the disease labels (could be single or multiple separated by |)\n",
    "        if pd.notna(finding_labels):\n",
    "            diseases = [d.strip() for d in str(finding_labels).split('|')]\n",
    "            for disease in diseases:\n",
    "                if disease in self.disease_classes:\n",
    "                    idx_disease = self.disease_classes.index(disease)\n",
    "                    label_vector[idx_disease] = 1.0\n",
    "        \n",
    "        return image, label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7030cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD DATA & TRANSFORMS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Disease classes (excluded: Cardiomegaly, Consolidation, Pleural_Thickening due to insufficient data)\n",
    "DISEASE_CLASSES = [\n",
    "    'Atelectasis', 'Effusion', 'Infiltration', 'Mass', \n",
    "    'No Finding', 'Nodule', 'Pneumothorax'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(DISEASE_CLASSES)\n",
    "print(f\"Classes: {DISEASE_CLASSES}\")\n",
    "\n",
    "# Data transforms with augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.2))\n",
    "])\n",
    "\n",
    "# Standard transforms for validation/test\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = 'C:/xray_data/data'\n",
    "\n",
    "# Create multi-label datasets\n",
    "train_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'train_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'train'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'val_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'val'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "test_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'test_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'test'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset):,}\")\n",
    "print(f\"  Val:   {len(val_dataset):,}\")\n",
    "print(f\"  Test:  {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3030b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CLASS WEIGHTS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    2.34, 2.06, 1.88, 3.74, 1.00, 3.02, 4.59\n",
    "], dtype=torch.float32)\n",
    "\n",
    "training_counts = [2871, 3266, 5022, 1419, 13365, 1564, 624]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLASS WEIGHTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Disease':<25} {'Weight':<10} {'Samples'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, disease in enumerate(DISEASE_CLASSES):\n",
    "    print(f\"{disease:<25} {class_weights[i].item():<10.2f} {training_counts[i]:,}\")\n",
    "\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# MIXUP AUGMENTATION\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Apply Mixup augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Compute loss for mixed samples\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# MODEL SETUP\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "pos_weights = class_weights.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "use_amp = device.type == 'cuda'\n",
    "scaler = GradScaler() if use_amp else None\n",
    "\n",
    "print(f\"Model: ResNet50 ({sum(p.numel() for p in model.parameters()):,} parameters)\")\n",
    "print(f\"Mixed Precision: {'Enabled' if use_amp else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a643973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TRAINING LOOP\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_amp:\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        mixed_labels = lam * labels_a + (1 - lam) * labels_b\n",
    "        correct += (preds == (mixed_labels > 0.5).float()).sum().item()\n",
    "        total += labels.numel()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.numel()\n",
    "            \n",
    "            all_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    val_auc = roc_auc_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val   - Acc: {val_acc:.2f}%, AUC: {val_auc:.4f} ({epoch_time/60:.1f} min)\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), 'best_resnet50.pth')\n",
    "        print(f\"Best model saved (val_acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\nEarly stopping after {patience} epochs without improvement\")\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining complete. Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TEST EVALUATION & OPTIMAL THRESHOLD TUNING\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"\\nEvaluating model on test set...\")\n",
    "\n",
    "model.load_state_dict(torch.load('best_resnet50.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        \n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "\n",
    "test_labels = np.vstack(test_labels)\n",
    "test_probs = np.vstack(test_probs)\n",
    "test_preds_default = (test_probs > 0.5).astype(int)\n",
    "\n",
    "exact_match_default = (test_labels == test_preds_default).all(axis=1).mean() * 100\n",
    "hamming_default = (test_labels == test_preds_default).mean() * 100\n",
    "auc = roc_auc_score(test_labels, test_probs, average='macro')\n",
    "_, _, f1s_default, _ = precision_recall_fscore_support(test_labels, test_preds_default, average=None, zero_division=0)\n",
    "avg_f1_default = f1s_default.mean()\n",
    "\n",
    "print(f\"\\nBaseline Performance (threshold=0.5):\")\n",
    "print(f\"  Exact Match: {exact_match_default:.2f}%\")\n",
    "print(f\"  Hamming:     {hamming_default:.2f}%\")\n",
    "print(f\"  AUC:         {auc:.4f}\")\n",
    "print(f\"  Avg F1:      {avg_f1_default:.3f}\")\n",
    "\n",
    "# Find optimal thresholds per class\n",
    "print(f\"\\nOptimizing thresholds per disease class...\")\n",
    "\n",
    "threshold_range = np.arange(0.05, 0.96, 0.05)\n",
    "optimal_thresholds = {}\n",
    "threshold_results = []\n",
    "\n",
    "for disease_idx, disease in enumerate(DISEASE_CLASSES):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    \n",
    "    true_labels = test_labels[:, disease_idx]\n",
    "    pred_probs = test_probs[:, disease_idx]\n",
    "    \n",
    "    for threshold in threshold_range:\n",
    "        predictions = (pred_probs > threshold).astype(int)\n",
    "        if predictions.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "        recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "        f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "    \n",
    "    optimal_thresholds[disease] = best_threshold\n",
    "    default_f1 = f1s_default[disease_idx]\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Disease': disease,\n",
    "        'Optimal_Threshold': best_threshold,\n",
    "        'Optimal_F1': best_f1,\n",
    "        'Optimal_Precision': best_precision,\n",
    "        'Optimal_Recall': best_recall,\n",
    "        'Default_F1': default_f1,\n",
    "        'Improvement': best_f1 - default_f1,\n",
    "        'Support': int(true_labels.sum())\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"OPTIMAL THRESHOLDS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Disease':<20} {'Threshold':<11} {'F1':<8} {'Precision':<11} {'Recall':<8} {'Gain'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for result in threshold_results:\n",
    "    improvement_str = f\"+{result['Improvement']:.3f}\" if result['Improvement'] > 0 else f\"{result['Improvement']:.3f}\"\n",
    "    print(f\"{result['Disease']:<20} {result['Optimal_Threshold']:<11.2f} \"\n",
    "          f\"{result['Optimal_F1']:<8.3f} {result['Optimal_Precision']:<11.3f} \"\n",
    "          f\"{result['Optimal_Recall']:<8.3f} {improvement_str}\")\n",
    "\n",
    "avg_optimal_f1 = sum(r['Optimal_F1'] for r in threshold_results) / len(threshold_results)\n",
    "avg_improvement = avg_optimal_f1 - avg_f1_default\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Average F1 - Default: {avg_f1_default:.3f}, Optimal: {avg_optimal_f1:.3f}\")\n",
    "print(f\"Improvement: +{avg_improvement:.3f} ({avg_improvement/avg_f1_default*100:+.1f}%)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d44cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# APPLY OPTIMAL THRESHOLDS & FINAL EVALUATION\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply optimal thresholds\n",
    "test_preds_optimal = np.zeros_like(test_labels)\n",
    "for disease_idx, disease in enumerate(DISEASE_CLASSES):\n",
    "    optimal_thresh = optimal_thresholds[disease]\n",
    "    test_preds_optimal[:, disease_idx] = (test_probs[:, disease_idx] > optimal_thresh).astype(int)\n",
    "\n",
    "exact_match_optimal = (test_labels == test_preds_optimal).all(axis=1).mean() * 100\n",
    "hamming_optimal = (test_labels == test_preds_optimal).mean() * 100\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Exact Match: {exact_match_optimal:.2f}%\")\n",
    "print(f\"  Hamming:     {hamming_optimal:.2f}%\")\n",
    "print(f\"  AUC:         {auc:.4f}\")\n",
    "print(f\"  Avg F1:      {avg_optimal_f1:.3f}\")\n",
    "\n",
    "# Per-class results\n",
    "optimal_precisions, optimal_recalls, optimal_f1s, supports = precision_recall_fscore_support(\n",
    "    test_labels, test_preds_optimal, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\nPer-Class F1 Scores:\")\n",
    "for i, disease in enumerate(DISEASE_CLASSES):\n",
    "    print(f\"  {disease:<20} {optimal_f1s[i]:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "x = np.arange(len(DISEASE_CLASSES))\n",
    "width = 0.35\n",
    "\n",
    "ax1.barh(x - width/2, f1s_default, width, label='Default (0.5)', color='steelblue', alpha=0.8)\n",
    "ax1.barh(x + width/2, optimal_f1s, width, label='Optimal', color='forestgreen', alpha=0.8)\n",
    "ax1.set_yticks(x)\n",
    "ax1.set_yticklabels(DISEASE_CLASSES)\n",
    "ax1.set_xlabel('F1-Score')\n",
    "ax1.set_title('Per-Class F1 Scores: Default vs Optimal Thresholds')\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "ax2.barh(DISEASE_CLASSES, [optimal_thresholds[d] for d in DISEASE_CLASSES], color='coral')\n",
    "ax2.axvline(x=0.5, color='gray', linestyle='--', label='Default (0.5)', linewidth=2)\n",
    "ax2.set_xlabel('Threshold Value')\n",
    "ax2.set_title('Optimal Thresholds by Disease')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('resnet50_performance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualization saved to 'resnet50_performance.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d037b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# INFERENCE ON SINGLE IMAGE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def predict_multilabel_image(image_path, model, device, transform, disease_classes, per_class_thresholds):\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.sigmoid(outputs).cpu().numpy()[0]\n",
    "    \n",
    "    predictions = []\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        disease = disease_classes[i]\n",
    "        threshold = per_class_thresholds.get(disease, 0.5)\n",
    "        \n",
    "        if prob > threshold:\n",
    "            predictions.append((disease, prob))\n",
    "    \n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return image, probabilities, predictions\n",
    "\n",
    "\n",
    "# Test inference\n",
    "idx = random.randint(0, len(test_dataset) - 1)\n",
    "row = test_dataset.metadata.iloc[idx]\n",
    "img_name = row['Image Index']\n",
    "\n",
    "img_path = None\n",
    "for class_folder in DISEASE_CLASSES:\n",
    "    path = os.path.join(test_dataset.image_dir, class_folder, img_name)\n",
    "    if os.path.exists(path):\n",
    "        img_path = path\n",
    "        break\n",
    "if img_path is None:\n",
    "    img_path = os.path.join(test_dataset.image_dir, img_name)\n",
    "\n",
    "print(f\"\\nTest Image: {img_name}\")\n",
    "\n",
    "# Ground truth\n",
    "ground_truth_labels = []\n",
    "finding_labels = row['Finding Labels']\n",
    "if pd.notna(finding_labels):\n",
    "    diseases = [d.strip() for d in str(finding_labels).split('|')]\n",
    "    ground_truth_labels = [d for d in diseases if d in DISEASE_CLASSES]\n",
    "\n",
    "print(\"\\nGround Truth:\")\n",
    "if ground_truth_labels:\n",
    "    for disease in ground_truth_labels:\n",
    "        print(f\"  {disease}\")\n",
    "else:\n",
    "    print(\"  No findings\")\n",
    "\n",
    "# Prediction\n",
    "original_image, all_probs, predicted_diseases = predict_multilabel_image(\n",
    "    img_path, model, device, eval_transform, DISEASE_CLASSES, optimal_thresholds\n",
    ")\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "if predicted_diseases:\n",
    "    for disease, prob in predicted_diseases:\n",
    "        thresh = optimal_thresholds.get(disease, 0.5)\n",
    "        print(f\"  {disease}: {prob*100:.1f}% (threshold: {thresh:.2f})\")\n",
    "else:\n",
    "    print(\"  No findings\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.imshow(original_image, cmap='gray')\n",
    "ax1.set_title(f'Chest X-Ray: {img_name}')\n",
    "ax1.axis('off')\n",
    "\n",
    "colors = ['green' if p > optimal_thresholds.get(DISEASE_CLASSES[i], 0.5) else 'lightgray' \n",
    "          for i, p in enumerate(all_probs)]\n",
    "ax2.barh(DISEASE_CLASSES, all_probs, color=colors)\n",
    "ax2.set_xlabel('Probability')\n",
    "ax2.set_title('Disease Probabilities (Green = Above Threshold)')\n",
    "ax2.set_xlim(0, 1)\n",
    "\n",
    "for i, disease in enumerate(DISEASE_CLASSES):\n",
    "    thresh = optimal_thresholds.get(disease, 0.5)\n",
    "    ax2.plot([thresh, thresh], [i-0.4, i+0.4], 'r--', linewidth=2, alpha=0.7)\n",
    "\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

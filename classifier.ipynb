{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4527184e",
   "metadata": {},
   "source": [
    "# Chest X-Ray Disease Classification (Multi-Label, Top 10 Diseases)\n",
    "\n",
    "Train an EfficientNet-B3 model for multi-label classification of the 10 most common chest disease categories from the NIH Chest X-Ray dataset.\n",
    "\n",
    "## Dataset\n",
    "- 31,416 chest X-ray images\n",
    "- Split: 21,991 train / 4,712 validation / 4,713 test\n",
    "- 10 disease classes (multi-label: each image can have 0-10 diseases)\n",
    "- **Excluded rare diseases**: Fibrosis, Hernia, Pleural_Thickening, Pneumonia, Pneumothorax (insufficient training samples)\n",
    "\n",
    "## Model\n",
    "- Architecture: EfficientNet-B3 pretrained on ImageNet\n",
    "- Classification Type: **Multi-Label** (BCEWithLogitsLoss)\n",
    "- Image Size: 300x300 (optimized for EfficientNet-B3)\n",
    "- Batch Size: 192 (with mixed precision)\n",
    "- Optimizer: AdamW with weight decay 0.01\n",
    "- Scheduler: CosineAnnealingWarmRestarts\n",
    "- Data augmentation: Mixup, horizontal flip, rotation, affine, color jitter, erasing\n",
    "- Mixed Precision: Enabled (AMP)\n",
    "\n",
    "## Results\n",
    "- Previous 15-class baseline: 45.55% exact match accuracy\n",
    "- Target 10-class accuracy: 55-65% (more focused on common diseases)\n",
    "- Training in progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7661e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# IMPORTS\n",
    "# Load required libraries for PyTorch training and evaluation\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import os \n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import timm  # For EfficientNet models\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed precision training\n",
    "import random  # For Mixup\n",
    "from tqdm.auto import tqdm  # Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39158fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DATASET CLASS - MULTI-LABEL\n",
    "# Custom PyTorch dataset for loading chest X-ray images with multi-label support\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class ChestXrayDatasetMultiLabel(Dataset):\n",
    "    \"\"\"Dataset for NIH Chest X-ray with 15 disease classes (multi-label)\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_csv, image_dir, disease_classes, transform=None):\n",
    "        self.metadata = pd.read_csv(metadata_csv)\n",
    "        self.image_dir = image_dir\n",
    "        self.disease_classes = disease_classes\n",
    "        self.transform = transform\n",
    "        self.num_classes = len(disease_classes)\n",
    "        \n",
    "        print(f\"Loaded {len(self.metadata)} samples from {metadata_csv}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        # Get image path - check both naming conventions\n",
    "        img_name = row['Image Index']\n",
    "        \n",
    "        # Try to find image in any of the class folders\n",
    "        img_path = None\n",
    "        for class_folder in self.disease_classes:\n",
    "            potential_path = os.path.join(self.image_dir, class_folder, img_name)\n",
    "            if os.path.exists(potential_path):\n",
    "                img_path = potential_path\n",
    "                break\n",
    "        \n",
    "        # If not found in class folders, try root\n",
    "        if img_path is None:\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Create multi-label vector (binary for each disease)\n",
    "        finding_labels = row['Finding Labels']\n",
    "        label_vector = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        \n",
    "        # Parse the disease labels (could be single or multiple separated by |)\n",
    "        if pd.notna(finding_labels):\n",
    "            diseases = [d.strip() for d in str(finding_labels).split('|')]\n",
    "            for disease in diseases:\n",
    "                if disease in self.disease_classes:\n",
    "                    idx_disease = self.disease_classes.index(disease)\n",
    "                    label_vector[idx_disease] = 1.0\n",
    "        \n",
    "        return image, label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7030cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD DATA & TRANSFORMS - MULTI-LABEL (TOP 10 DISEASES)\n",
    "# Load class mapping and create train/val/test datasets with transforms\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Define TOP 10 most common disease classes (excludes rare diseases)\n",
    "# NOTE: \"No Finding\" has a SPACE not underscore in the CSV!\n",
    "# Excluded: Edema, Emphysema, Fibrosis, Hernia, Pneumonia\n",
    "DISEASE_CLASSES = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Effusion', 'Infiltration',\n",
    "    'Mass', 'No Finding', 'Nodule', 'Pleural_Thickening', 'Pneumothorax'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(DISEASE_CLASSES)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Classes: {DISEASE_CLASSES}\")\n",
    "print(f\"\\nUsing MULTI-LABEL classification with TOP 10 diseases\")\n",
    "print(f\"Excluded rare diseases: Edema, Emphysema, Fibrosis, Hernia, Pneumonia\")\n",
    "\n",
    "# Data transforms with augmentation for training\n",
    "# EfficientNet-B3 performs better with 300x300 images\n",
    "# Enhanced augmentation for more robust training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip X-rays horizontally\n",
    "    transforms.RandomRotation(degrees=10),    # Slight rotation\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random shifts\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.2))  # Randomly erase patches\n",
    "])\n",
    "\n",
    "# Standard transforms for validation/test (no augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Data directory (local SSD location, outside OneDrive)\n",
    "DATA_DIR = 'C:/xray_data/data'  # Updated to include 'data' subdirectory\n",
    "\n",
    "# Create multi-label datasets\n",
    "train_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'train_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'train'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'val_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'val'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "test_dataset = ChestXrayDatasetMultiLabel(\n",
    "    metadata_csv=os.path.join(DATA_DIR, 'test_metadata.csv'),\n",
    "    image_dir=os.path.join(DATA_DIR, 'test'),\n",
    "    disease_classes=DISEASE_CLASSES,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset):,} images\")\n",
    "print(f\"Val:   {len(val_dataset):,} images\")\n",
    "print(f\"Test:  {len(test_dataset):,} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6682b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CLASS DISTRIBUTION (HARDCODED - PRE-CALCULATED)\n",
    "# Counts of disease occurrences in each dataset split\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# HARDCODED DISTRIBUTION \n",
    "# Training Set (28,153 images)\n",
    "TRAIN_DISTRIBUTION = {\n",
    "    'No Finding': 13365,\n",
    "    'Infiltration': 5022,\n",
    "    'Effusion': 3266,\n",
    "    'Atelectasis': 2871,\n",
    "    'Nodule': 1564,\n",
    "    'Mass': 1419,\n",
    "    'Consolidation': 1204,\n",
    "    'Pneumothorax': 624,\n",
    "    'Cardiomegaly': 664,\n",
    "    'Pleural_Thickening': 160\n",
    "}\n",
    "\n",
    "# Validation Set (5,887 images)\n",
    "VAL_DISTRIBUTION = {\n",
    "    'No Finding': 2801,\n",
    "    'Infiltration': 1043,\n",
    "    'Effusion': 680,\n",
    "    'Atelectasis': 622,\n",
    "    'Nodule': 320,\n",
    "    'Mass': 305,\n",
    "    'Consolidation': 228,\n",
    "    'Pneumothorax': 144,\n",
    "    'Cardiomegaly': 144,\n",
    "    'Pleural_Thickening': 214\n",
    "}\n",
    "\n",
    "# Test Set (5,960 images)\n",
    "TEST_DISTRIBUTION = {\n",
    "    'No Finding': 2842,\n",
    "    'Infiltration': 1055,\n",
    "    'Effusion': 689,\n",
    "    'Atelectasis': 595,\n",
    "    'Nodule': 324,\n",
    "    'Mass': 292,\n",
    "    'Consolidation': 242,\n",
    "    'Pneumothorax': 111,\n",
    "    'Cardiomegaly': 111,\n",
    "    'Pleural_Thickening': 160\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASS DISTRIBUTION (Hardcoded - Pre-calculated)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTraining Set (28,153 images):\")\n",
    "for disease in DISEASE_CLASSES:\n",
    "    count = TRAIN_DISTRIBUTION[disease]\n",
    "    percentage = (count / len(train_dataset)) * 100\n",
    "    print(f\"  {disease:<25} {count:>6,} ({percentage:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nValidation Set (5,887 images):\")\n",
    "for disease in DISEASE_CLASSES:\n",
    "    count = VAL_DISTRIBUTION[disease]\n",
    "    percentage = (count / len(val_dataset)) * 100\n",
    "    print(f\"  {disease:<25} {count:>6,} ({percentage:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nTest Set (5,960 images):\")\n",
    "for disease in DISEASE_CLASSES:\n",
    "    count = TEST_DISTRIBUTION[disease]\n",
    "    percentage = (count / len(test_dataset)) * 100\n",
    "    print(f\"  {disease:<25} {count:>6,} ({count:>5.2f}%)\")\n",
    "\n",
    "print(\"\\n✓ Distribution loaded instantly (no computation needed!)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# ORIGINAL DISTRIBUTION ANALYSIS (COMMENTED OUT - USE IF DATA CHANGES)\n",
    "# ------------------------------------------------------------------------------\n",
    "# def analyze_class_distribution(dataset, dataset_name):\n",
    "#     \"\"\"Count disease frequencies in the dataset\"\"\"\n",
    "#     disease_counts = {disease: 0 for disease in DISEASE_CLASSES}\n",
    "#     \n",
    "#     # Count occurrences\n",
    "#     for idx in range(len(dataset)):\n",
    "#         _, label_vector = dataset[idx]\n",
    "#         for i, disease in enumerate(DISEASE_CLASSES):\n",
    "#             if label_vector[i] == 1.0:\n",
    "#                 disease_counts[disease] += 1\n",
    "#     \n",
    "#     # Sort by count\n",
    "#     sorted_diseases = sorted(disease_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "#     \n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"{dataset_name} - Disease Distribution\")\n",
    "#     print(f\"{'='*70}\")\n",
    "#     print(f\"{'Disease':<25} {'Count':<10} {'Percentage':<15} {'Imbalance Ratio':<15}\")\n",
    "#     print(\"-\" * 70)\n",
    "#     \n",
    "#     max_count = sorted_diseases[0][1] if sorted_diseases[0][1] > 0 else 1\n",
    "#     total_images = len(dataset)\n",
    "#     \n",
    "#     for disease, count in sorted_diseases:\n",
    "#         percentage = (count / total_images) * 100 if total_images > 0 else 0\n",
    "#         ratio = max_count / count if count > 0 else float('inf')\n",
    "#         print(f\"{disease:<25} {count:<10} {percentage:<14.2f}% {ratio:<14.1f}x\")\n",
    "#     \n",
    "#     print(f\"\\nTotal images: {total_images:,}\")\n",
    "#     print(f\"Most common: {sorted_diseases[0][0]} ({sorted_diseases[0][1]:,} images)\")\n",
    "#     print(f\"Least common: {sorted_diseases[-1][0]} ({sorted_diseases[-1][1]:,} images)\")\n",
    "#     \n",
    "#     # Avoid division by zero\n",
    "#     if sorted_diseases[-1][1] > 0:\n",
    "#         print(f\"Imbalance ratio (max/min): {max_count / sorted_diseases[-1][1]:.1f}:1\")\n",
    "#     else:\n",
    "#         print(f\"Imbalance ratio (max/min): Infinite (some classes have 0 samples)\")\n",
    "# \n",
    "# # Analyze all three datasets\n",
    "# analyze_class_distribution(train_dataset, \"TRAINING SET\")\n",
    "# analyze_class_distribution(val_dataset, \"VALIDATION SET\")\n",
    "# analyze_class_distribution(test_dataset, \"TEST SET\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6823ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CLASS WEIGHTS (HARDCODED - PRE-CALCULATED FROM TRAINING DATA)\n",
    "# Using square root transformation + capping for gentle balancing\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# HARDCODED WEIGHTS - calculated once, reused forever!\n",
    "# Based on training set: 28,153 images\n",
    "# Formula: sqrt(neg_count / pos_count), capped at [1.0, 8.0]\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    2.34,  # Atelectasis (2,871 samples)\n",
    "    5.03,  # Cardiomegaly (664 samples)\n",
    "    4.06,  # Consolidation (1,204 samples)\n",
    "    2.06,  # Effusion (3,266 samples)\n",
    "    1.88,  # Infiltration (5,022 samples)\n",
    "    3.74,  # Mass (1,419 samples)\n",
    "    1.00,  # No Finding (13,365 samples - baseline, no weighting)\n",
    "    3.02,  # Nodule (1,564 samples)\n",
    "    8.00,  # Pleural_Thickening (160 samples - rare, max weight)\n",
    "    4.59,  # Pneumothorax (624 samples)\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLASS WEIGHTS (Hardcoded - Pre-calculated)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Disease':<25} {'Weight':<10} {'Strategy'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "weight_descriptions = [\n",
    "    (\"Atelectasis\", 2.34, \"Moderate boost\"),\n",
    "    (\"Cardiomegaly\", 5.03, \"Strong boost (rare)\"),\n",
    "    (\"Consolidation\", 4.06, \"Strong boost\"),\n",
    "    (\"Effusion\", 2.06, \"Light boost (common)\"),\n",
    "    (\"Infiltration\", 1.88, \"Light boost (very common)\"),\n",
    "    (\"Mass\", 3.74, \"Strong boost\"),\n",
    "    (\"No Finding\", 1.00, \"No boost (baseline)\"),\n",
    "    (\"Nodule\", 3.02, \"Moderate boost\"),\n",
    "    (\"Pleural_Thickening\", 8.00, \"MAX boost (very rare)\"),\n",
    "    (\"Pneumothorax\", 4.59, \"Strong boost\"),\n",
    "]\n",
    "\n",
    "for disease, weight, strategy in weight_descriptions:\n",
    "    print(f\"{disease:<25} {weight:<10.2f} {strategy}\")\n",
    "\n",
    "print(f\"\\nWeight strategy:\")\n",
    "print(f\"  ✓ Pre-calculated from training data (28,153 images)\")\n",
    "print(f\"  ✓ Square root transformation applied\")\n",
    "print(f\"  ✓ Capped at [1.0, 8.0] range\")\n",
    "print(f\"  ✓ INSTANT - no computation needed!\")\n",
    "print(f\"\\n✓ Class weights ready to use!\")\n",
    "\n",
    "# in case data changes\n",
    "# def calculate_gentle_class_weights_fast(dataset, disease_classes):\n",
    "#     \"\"\"Use this if you need to recalculate weights\"\"\"\n",
    "#     disease_counts = {}\n",
    "#     for disease in disease_classes:\n",
    "#         if disease in dataset.metadata.columns:\n",
    "#             disease_counts[disease] = int(dataset.metadata[disease].sum())\n",
    "#         else:\n",
    "#             count = dataset.metadata['Finding Labels'].str.contains(disease, regex=False, na=False).sum()\n",
    "#             disease_counts[disease] = int(count)\n",
    "#     \n",
    "#     total_samples = len(dataset)\n",
    "#     pos_counts = torch.tensor([disease_counts[disease] for disease in disease_classes], dtype=torch.float32)\n",
    "#     neg_counts = total_samples - pos_counts\n",
    "#     standard_weights = neg_counts / (pos_counts + 1e-6)\n",
    "#     gentle_weights = torch.sqrt(standard_weights)\n",
    "#     gentle_weights = torch.clamp(gentle_weights, min=1.0, max=8.0)\n",
    "#     return gentle_weights\n",
    "# \n",
    "# # Recalculate: class_weights = calculate_gentle_class_weights_fast(train_dataset, DISEASE_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# MIXUP AUGMENTATION - MULTI-LABEL\n",
    "# Blends two images and their labels for better generalization\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Apply Mixup augmentation for multi-label\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Compute loss for mixed samples (multi-label compatible)\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# MODEL SETUP\n",
    "# Create DataLoaders and initialize EfficientNet-B3\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Batch size - adjust based on available memory\n",
    "BATCH_SIZE = 32  # Universal batch size that works on most devices\n",
    "NUM_WORKERS = 0  # Set to 0 for compatibility\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Device configuration - works on any device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Model: EfficientNet-B3\n",
    "model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function for multi-label classification WITH GENTLE CLASS WEIGHTS\n",
    "pos_weight = class_weights.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "print(f\"\\n✓ Using weighted loss function:\")\n",
    "print(f\"  Weights applied: {class_weights.cpu().numpy()}\")\n",
    "print(f\"  Range: {class_weights.min():.2f} to {class_weights.max():.2f}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "# Mixed precision training (only if CUDA is available)\n",
    "use_amp = device.type == 'cuda'\n",
    "scaler = GradScaler() if use_amp else None\n",
    "\n",
    "print(f\"\\nModel: EfficientNet-B3 (Multi-Label)\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Mixed Precision: {'Enabled' if use_amp else 'Disabled (CPU mode)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a643973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TRAINING LOOP - MULTI-LABEL\n",
    "# Train EfficientNet-B3 with Mixed Precision, Mixup, and Cosine Annealing\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Total batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Track batch timing\n",
    "    batch_times = []\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Apply Mixup augmentation\n",
    "        images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (with or without mixed precision)\n",
    "        if use_amp:\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Multi-label accuracy\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        mixed_labels = lam * labels_a + (1 - lam) * labels_b\n",
    "        correct += (preds == (mixed_labels > 0.5).float()).sum().item()\n",
    "        total += labels.numel()\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        # Print every 20 batches\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            elapsed = time.time() - epoch_start\n",
    "            avg_time = np.mean(batch_times[-20:])\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f} | {avg_time:.2f}s/batch | Elapsed: {elapsed/60:.1f}min\")\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_batch_time = np.mean(batch_times)\n",
    "    \n",
    "    print(f\"\\nTraining Results:\")\n",
    "    print(f\"  Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"  Avg batch time: {avg_batch_time:.3f}s | Throughput: {BATCH_SIZE/avg_batch_time:.1f} img/s\")\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"\\nRunning validation...\")\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Apply sigmoid and threshold for multi-label\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.numel()\n",
    "            \n",
    "            # Store for AUC calculation\n",
    "            all_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"  Validation batch {batch_idx+1}/{len(val_loader)}\")\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    # Calculate mean AUC-ROC\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    try:\n",
    "        val_auc = roc_auc_score(all_labels, all_preds, average='macro')\n",
    "    except:\n",
    "        val_auc = 0.0\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  Accuracy: {val_acc:.2f}% | AUC: {val_auc:.4f}\")\n",
    "    print(f\"\\nEpoch completed in {epoch_time/60:.1f} minutes\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_efficientnet_b3.pth')\n",
    "        print(f\"✓ New best model saved! (val_acc: {val_acc:.2f}%)\\n\")\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4618e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TEST EVALUATION - MULTI-LABEL\n",
    "# Load best model and evaluate on test set\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_efficientnet_b3.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Multi-label: apply sigmoid and threshold\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        \n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "        test_preds.append(preds.cpu().numpy())\n",
    "\n",
    "# Stack all predictions\n",
    "test_labels = np.vstack(test_labels)\n",
    "test_preds = np.vstack(test_preds)\n",
    "\n",
    "# Overall accuracy (exact match)\n",
    "exact_match_acc = (test_labels == test_preds).all(axis=1).mean() * 100\n",
    "\n",
    "# Hamming accuracy (per-label accuracy)\n",
    "hamming_acc = (test_labels == test_preds).mean() * 100\n",
    "\n",
    "# AUC-ROC\n",
    "try:\n",
    "    test_auc = roc_auc_score(test_labels, test_preds, average='macro')\n",
    "except:\n",
    "    test_auc = 0.0\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Exact Match Accuracy: {exact_match_acc:.2f}% (all labels correct)\")\n",
    "print(f\"  Hamming Accuracy: {hamming_acc:.2f}% (per-label accuracy)\")\n",
    "print(f\"  AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(f\"{'Disease':<25} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precisions, recalls, f1s, supports = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "for i, disease in enumerate(DISEASE_CLASSES):\n",
    "    print(f\"{disease:<25} {precisions[i]:<10.3f} {recalls[i]:<10.3f} {f1s[i]:<10.3f} {int(supports[i]):<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# VISUALIZE RESULTS - MULTI-LABEL\n",
    "# Generate and save per-class performance visualization\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create bar chart of per-class F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(DISEASE_CLASSES, f1s, color='steelblue')\n",
    "plt.xlabel('F1-Score')\n",
    "plt.title('Per-Class F1-Scores - EfficientNet-B3 (Multi-Label)')\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_f1_scores.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualization saved to 'per_class_f1_scores.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08190e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# INFERENCE ON SINGLE IMAGE - MULTI-LABEL\n",
    "# Test the trained model on a chest X-ray image\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def predict_multilabel_image(image_path, model, device, transform, disease_classes, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict multiple diseases for a chest X-ray image\n",
    "    Returns predicted diseases with their probabilities\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.sigmoid(outputs).cpu().numpy()[0]\n",
    "    \n",
    "    # Get all diseases above threshold\n",
    "    predictions = []\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        if prob > threshold:\n",
    "            predictions.append((disease_classes[i], prob))\n",
    "    \n",
    "    # Sort by probability (highest first)\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return image, probabilities, predictions\n",
    "\n",
    "\n",
    "# Example usage: Test on a random image from test set\n",
    "import random\n",
    "\n",
    "# Get a random test image from the metadata\n",
    "random_idx = random.randint(0, len(test_dataset) - 1)\n",
    "test_row = test_dataset.metadata.iloc[random_idx]\n",
    "test_image_name = test_row['Image Index']\n",
    "\n",
    "# Build the full path\n",
    "test_image_path = None\n",
    "for class_folder in DISEASE_CLASSES:\n",
    "    potential_path = os.path.join(test_dataset.image_dir, class_folder, test_image_name)\n",
    "    if os.path.exists(potential_path):\n",
    "        test_image_path = potential_path\n",
    "        break\n",
    "if test_image_path is None:\n",
    "    test_image_path = os.path.join(test_dataset.image_dir, test_image_name)\n",
    "\n",
    "print(f\"Testing on: {test_image_name}\")\n",
    "print(f\"Ground truth: {test_row['Finding Labels']}\\n\")\n",
    "\n",
    "# Make prediction\n",
    "original_image, all_probs, predicted_diseases = predict_multilabel_image(\n",
    "    test_image_path, \n",
    "    model, \n",
    "    device, \n",
    "    eval_transform,\n",
    "    DISEASE_CLASSES,\n",
    "    threshold=0.3  # Lower threshold to see more predictions\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"Predicted Diseases (threshold > 0.3):\")\n",
    "print(\"-\" * 60)\n",
    "if len(predicted_diseases) > 0:\n",
    "    for disease, prob in predicted_diseases:\n",
    "        print(f\"  {disease:<25} - {prob*100:.2f}%\")\n",
    "else:\n",
    "    print(\"  No diseases detected (all probabilities below threshold)\")\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Show image\n",
    "ax1.imshow(original_image, cmap='gray')\n",
    "ax1.set_title('Input Chest X-Ray')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Show all disease probabilities\n",
    "ax2.barh(DISEASE_CLASSES, all_probs, color=['red' if p > 0.3 else 'lightblue' for p in all_probs])\n",
    "ax2.set_xlabel('Probability')\n",
    "ax2.set_title('Multi-Label Disease Predictions')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.axvline(x=0.3, color='red', linestyle='--', linewidth=1, label='Threshold (0.3)')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

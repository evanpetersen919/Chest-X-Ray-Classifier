{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4527184e",
   "metadata": {},
   "source": [
    "# Chest X-Ray Disease Classification\n",
    "\n",
    "Train a ResNet50 model to classify 15 different chest disease categories from the NIH Chest X-Ray dataset.\n",
    "\n",
    "## Dataset\n",
    "- 31,416 chest X-ray images\n",
    "- Split: 21,991 train / 4,712 validation / 4,713 test\n",
    "- 15 disease classes\n",
    "\n",
    "## Model\n",
    "- Architecture: ResNet50 pretrained on ImageNet\n",
    "- Optimizer: Adam with learning rate 0.001\n",
    "- Scheduler: ReduceLROnPlateau\n",
    "- Data augmentation: horizontal flip, rotation, color jitter\n",
    "\n",
    "## Results\n",
    "- Best validation accuracy: 37% after 20 epochs\n",
    "- Model saved as best_resnet50.pth\n",
    "- Confusion matrix visualization included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7661e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# IMPORTS\n",
    "# Load required libraries for PyTorch training and evaluation\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import os \n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39158fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DATASET CLASS\n",
    "# Custom PyTorch dataset for loading chest X-ray images\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"Dataset for NIH Chest X-ray with 15 disease classes\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None, cache_images=True):\n",
    "        self.root_dir = root_dir \n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.cache_images = cache_images\n",
    "        self.cached_data = {}\n",
    "\n",
    "        # Get all class folders (15 disease categories)\n",
    "        class_folders = sorted(os.listdir(root_dir))\n",
    "        \n",
    "        for class_idx, class_name in enumerate(class_folders):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            \n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            \n",
    "            # Collect all images in this class folder\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.endswith('.png'):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                    self.labels.append(class_idx)\n",
    "        \n",
    "        # Preload images into memory for faster training\n",
    "        if self.cache_images:\n",
    "            print(f\"Caching {len(self.image_paths)} images in memory...\")\n",
    "            for idx in range(len(self.image_paths)):\n",
    "                img_path = self.image_paths[idx]\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                self.cached_data[idx] = image\n",
    "            print(f\"\u2713 Cached {len(self.cached_data)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        # Load from cache if available, otherwise from disk\n",
    "        if self.cache_images and idx in self.cached_data:\n",
    "            image = self.cached_data[idx].copy()\n",
    "        else:\n",
    "            img_path = self.image_paths[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7030cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD DATA & TRANSFORMS\n",
    "# Load class mapping and create train/val/test datasets with transforms\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Load class mapping\n",
    "with open('data/class_mapping.json', 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "    class_mapping = {int(k): v for k, v in class_mapping.items()}\n",
    "\n",
    "NUM_CLASSES = len(class_mapping)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Classes: {list(class_mapping.values())}\")\n",
    "\n",
    "# Data transforms with augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip X-rays horizontally\n",
    "    transforms.RandomRotation(degrees=10),    # Slight rotation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Standard transforms for validation/test (no augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChestXrayDataset(root_dir='data/train', transform=train_transform, cache_images=False)\n",
    "val_dataset = ChestXrayDataset(root_dir='data/val', transform=eval_transform, cache_images=False)\n",
    "test_dataset = ChestXrayDataset(root_dir='data/test', transform=eval_transform, cache_images=False)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset):,} images\")\n",
    "print(f\"Val:   {len(val_dataset):,} images\")\n",
    "print(f\"Test:  {len(test_dataset):,} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# MODEL SETUP\n",
    "# Create DataLoaders and initialize ResNet50 with GPU acceleration\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True  # Speed optimization\n",
    "\n",
    "# Model: ResNet50\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "print(f\"\\nModel: ResNet50\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a643973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TRAINING LOOP\n",
    "# Train ResNet50 for 20 epochs with validation and best model checkpointing\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:  # Print progress every 50 batches\n",
    "            print(f\"  Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_resnet50.pth')\n",
    "        print(f\"Saved best model (val_acc: {val_acc:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4618e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TEST EVALUATION\n",
    "# Load best model and evaluate on test set with per-class accuracy\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_resnet50.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i in range(NUM_CLASSES):\n",
    "    class_mask = np.array(test_labels) == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(\n",
    "            np.array(test_labels)[class_mask],\n",
    "            np.array(test_preds)[class_mask]\n",
    "        )\n",
    "        print(f\"  {class_mapping[i]:25} - {class_acc*100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# VISUALIZE RESULTS\n",
    "# Generate and save confusion matrix heatmap for model performance analysis\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[class_mapping[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[class_mapping[i] for i in range(NUM_CLASSES)])\n",
    "plt.title('Confusion Matrix - ResNet50')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_resnet50.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nConfusion matrix saved to 'confusion_matrix_resnet50.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08190e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# INFERENCE ON SINGLE IMAGE\n",
    "# Test the trained model on a new chest X-ray image\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def predict_image(image_path, model, device, transform, class_mapping, top_k=3):\n",
    "    \"\"\"\n",
    "    Predict disease class for a single chest X-ray image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        model: Trained PyTorch model\n",
    "        device: torch device (cuda/cpu)\n",
    "        transform: Image transforms\n",
    "        class_mapping: Dictionary mapping class indices to disease names\n",
    "        top_k: Number of top predictions to return\n",
    "    \n",
    "    Returns:\n",
    "        predictions: List of (disease_name, probability) tuples\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "    \n",
    "    # Format results\n",
    "    predictions = []\n",
    "    for prob, idx in zip(top_probs[0], top_indices[0]):\n",
    "        disease_name = class_mapping[idx.item()]\n",
    "        predictions.append((disease_name, prob.item()))\n",
    "    \n",
    "    return image, predictions\n",
    "\n",
    "\n",
    "# Example usage: Test on a random image from test set\n",
    "import random\n",
    "\n",
    "# Get a random test image\n",
    "test_image_path = random.choice(test_dataset.image_paths)\n",
    "print(f\"Testing on: {test_image_path}\\n\")\n",
    "\n",
    "# Make prediction\n",
    "original_image, predictions = predict_image(\n",
    "    test_image_path, \n",
    "    model, \n",
    "    device, \n",
    "    eval_transform,  # Use eval_transform (no augmentation for inference)\n",
    "    class_mapping, \n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Top 5 Predictions:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (disease, prob) in enumerate(predictions, 1):\n",
    "    print(f\"{i}. {disease:25} - {prob*100:.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image, cmap='gray')\n",
    "plt.title(f'Input X-ray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "diseases = [pred[0] for pred in predictions]\n",
    "probs = [pred[1] * 100 for pred in predictions]\n",
    "plt.barh(diseases, probs, color='steelblue')\n",
    "plt.xlabel('Confidence (%)')\n",
    "plt.title('Top 5 Predictions')\n",
    "plt.xlim(0, 100)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# Load NIH Chest X-ray dataset labels from CSV\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file that contains all the labels\n",
    "csv_path = r'archive (1)\\Data_Entry_2017.csv'\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease Categories: 15\n",
      "  [ 0] Atelectasis               - 11,559 images\n",
      "  [ 1] Cardiomegaly              -  2,776 images\n",
      "  [ 2] Consolidation             -  4,667 images\n",
      "  [ 3] Edema                     -  2,303 images\n",
      "  [ 4] Effusion                  - 13,317 images\n",
      "  [ 5] Emphysema                 -  2,516 images\n",
      "  [ 6] Fibrosis                  -  1,686 images\n",
      "  [ 7] Hernia                    -    227 images\n",
      "  [ 8] Infiltration              - 19,894 images\n",
      "  [ 9] Mass                      -  5,782 images\n",
      "  [10] No Finding                - 60,361 images\n",
      "  [11] Nodule                    -  6,331 images\n",
      "  [12] Pleural_Thickening        -  3,385 images\n",
      "  [13] Pneumonia                 -  1,431 images\n",
      "  [14] Pneumothorax              -  5,302 images\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# PREPARE MULTI-CLASS CLASSIFICATION DATA (15 CLASSES)\n",
    "# Extract and count all 15 disease categories from the dataset\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Get all 15 disease categories (excluding multi-disease combinations)\n",
    "all_diseases = []\n",
    "for labels in df['Finding Labels']:\n",
    "    diseases = labels.split('|')\n",
    "    all_diseases.extend(diseases)\n",
    "\n",
    "disease_counts = Counter(all_diseases)\n",
    "\n",
    "# Define the 15 classes (sorted alphabetically)\n",
    "CLASS_NAMES = sorted([d for d in disease_counts.keys()])\n",
    "\n",
    "print(f\"Disease Categories: {len(CLASS_NAMES)}\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    count = disease_counts[class_name]\n",
    "    print(f\"  [{i:2}] {class_name:25} - {count:6,} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5f98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 112,120\n",
      "Single-label: 91,324\n",
      "Multi-label: 20,796\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# FILTER SINGLE-LABEL IMAGES\n",
    "# Remove multi-label images to ensure clean single-class classification\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "single_label_df = df[~df['Finding Labels'].str.contains('|', regex=False)].copy()\n",
    "\n",
    "# Add numeric class labels\n",
    "single_label_df['Class_Label'] = single_label_df['Finding Labels'].apply(\n",
    "    lambda x: CLASS_NAMES.index(x)\n",
    ")\n",
    "\n",
    "print(f\"Total images: {len(df):,}\")\n",
    "print(f\"Single-label: {len(single_label_df):,}\")\n",
    "print(f\"Multi-label: {len(df) - len(single_label_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4ae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box annotations: 984\n",
      "Images with annotations: 256\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD SEGMENTATION DATA (BOUNDING BOXES)\n",
    "# Load bounding box annotations for future segmentation tasks\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "bbox_path = r'archive (1)\\BBox_List_2017.csv'\n",
    "bbox_df = pd.read_csv(bbox_path)\n",
    "\n",
    "# Merge with main dataframe\n",
    "single_label_df['Has_BBox'] = single_label_df['Image Index'].isin(bbox_df['Image Index'])\n",
    "\n",
    "print(f\"Bounding box annotations: {len(bbox_df):,}\")\n",
    "print(f\"Images with annotations: {single_label_df['Has_BBox'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28994f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset: 31,416 images (15 classes)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# BALANCE THE DATASET (HANDLE CLASS IMBALANCE)\n",
    "# Undersample majority classes to prevent model bias toward common diseases\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Balance dataset by limiting samples per class\n",
    "MAX_SAMPLES_PER_CLASS = 5000\n",
    "\n",
    "balanced_dfs = []\n",
    "for class_name in CLASS_NAMES:\n",
    "    class_data = single_label_df[single_label_df['Finding Labels'] == class_name]\n",
    "    \n",
    "    if len(class_data) > MAX_SAMPLES_PER_CLASS:\n",
    "        class_data = resample(class_data, n_samples=MAX_SAMPLES_PER_CLASS, random_state=42)\n",
    "    \n",
    "    balanced_dfs.append(class_data)\n",
    "\n",
    "balanced_df = pd.concat(balanced_dfs)\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Balanced dataset: {len(balanced_df):,} images ({len(CLASS_NAMES)} classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ab9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 21,991 images (171 with bboxes)\n",
      "Val:   4,712 images (43 with bboxes)\n",
      "Test:  4,713 images (36 with bboxes)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CREATE TRAIN/VALIDATION/TEST SPLITS\n",
    "# Stratified split: 70% train, 15% validation, 15% test\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(balanced_df, test_size=0.3, \n",
    "                                      stratify=balanced_df['Finding Labels'], \n",
    "                                      random_state=42)\n",
    "\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, \n",
    "                                    stratify=temp_df['Finding Labels'], \n",
    "                                    random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df):,} images ({train_df['Has_BBox'].sum():,} with bboxes)\")\n",
    "print(f\"Val:   {len(val_df):,} images ({val_df['Has_BBox'].sum():,} with bboxes)\")\n",
    "print(f\"Test:  {len(test_df):,} images ({test_df['Has_BBox'].sum():,} with bboxes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd2996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder structure: data/{train,val,test}/{15 classes}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CREATE FOLDER STRUCTURE (15 CLASSES)\n",
    "# Build directory hierarchy for PyTorch ImageFolder DataLoader\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path('data')\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for class_name in CLASS_NAMES:\n",
    "        safe_name = class_name.replace(' ', '_')\n",
    "        (base_dir / split / safe_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Created folder structure: data/{{train,val,test}}/{{15 classes}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54371e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already organized: 21287 images found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# ORGANIZE FILES (PARALLEL PROCESSING)\n",
    "# Copy images to class folders using multi-threaded processing for speed\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from functools import lru_cache\n",
    "import glob\n",
    "\n",
    "# Check if data is already organized\n",
    "if (base_dir / 'train').exists() and len(list((base_dir / 'train').rglob('*.png'))) > 0:\n",
    "    print(f\"Data already organized: {len(list((base_dir / 'train').rglob('*.png')))} images found\")\n",
    "else:\n",
    "    # Cache the find_image results to avoid repeated glob searches\n",
    "    @lru_cache(maxsize=None)\n",
    "    def find_image_cached(image_name):\n",
    "        \"\"\"Find which folder contains a specific image (cached)\"\"\"\n",
    "        pattern = f\"archive (1)/images_*/images/{image_name}\"\n",
    "        matches = glob.glob(pattern)\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "        return None\n",
    "    \n",
    "    def copy_single_image(args):\n",
    "        \"\"\"Copy a single image (for parallel processing)\"\"\"\n",
    "        image_name, class_label, split_name = args\n",
    "        \n",
    "        # Find source image\n",
    "        source_path = find_image_cached(image_name)\n",
    "        if source_path is None:\n",
    "            return f\"Warning: Could not find {image_name}\"\n",
    "        \n",
    "        # Copy to destination\n",
    "        dest_path = base_dir / split_name / class_label / image_name\n",
    "        try:\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            return None  # Success\n",
    "        except Exception as e:\n",
    "            return f\"Error copying {image_name}: {e}\"\n",
    "    \n",
    "    def copy_images_to_folders_parallel(dataframe, split_name, max_workers=8):\n",
    "        \"\"\"Copy images using parallel processing for speed\"\"\"\n",
    "        # Prepare arguments for parallel processing\n",
    "        args_list = [\n",
    "            (row['Image Index'], row['Finding Labels'].replace(' ', '_'), split_name)\n",
    "            for _, row in dataframe.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Use ThreadPoolExecutor for I/O-bound file copying\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Process files in parallel with progress bar\n",
    "            results = list(tqdm(\n",
    "                executor.map(copy_single_image, args_list),\n",
    "                total=len(args_list),\n",
    "                desc=f\"{split_name}\"\n",
    "            ))\n",
    "            # Collect errors\n",
    "            errors = [r for r in results if r is not None]\n",
    "        \n",
    "        return len(dataframe) - len(errors), len(errors)\n",
    "    \n",
    "    # Copy all splits with parallel processing\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_success, train_errors = copy_images_to_folders_parallel(train_df, 'train', max_workers=8)\n",
    "    val_success, val_errors = copy_images_to_folders_parallel(val_df, 'val', max_workers=8)\n",
    "    test_success, test_errors = copy_images_to_folders_parallel(test_df, 'test', max_workers=8)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    total_copied = train_success + val_success + test_success\n",
    "    \n",
    "    print(f\"\\nOrganized {total_copied:,} images in {elapsed:.1f}s ({total_copied/elapsed:.0f} img/s)\")\n",
    "    if train_errors + val_errors + test_errors > 0:\n",
    "        print(f\"Errors: {train_errors + val_errors + test_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01655567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# SAVE SEGMENTATION ANNOTATIONS\n",
    "# Export bounding box data to JSON for each split\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "# Save bounding box annotations for each split\n",
    "for split_name, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "    # Get images with bounding boxes\n",
    "    images_with_bbox = split_df[split_df['Has_BBox']]['Image Index'].tolist()\n",
    "    \n",
    "    # Filter bbox data for this split\n",
    "    split_bbox = bbox_df[bbox_df['Image Index'].isin(images_with_bbox)]\n",
    "    \n",
    "    bbox_dict = {}\n",
    "    for _, row in split_bbox.iterrows():\n",
    "        img_name = row['Image Index']\n",
    "        if img_name not in bbox_dict:\n",
    "            bbox_dict[img_name] = []\n",
    "        bbox_dict[img_name].append({\n",
    "            'class': row['Finding Label'],\n",
    "            'x': row['Bbox [x'],\n",
    "            'y': row['y'],\n",
    "            'w': row['w'],\n",
    "            'h': row['h]']\n",
    "        })\n",
    "    \n",
    "    # Save to file\n",
    "    with open(f'data/{split_name}_bboxes.json', 'w') as f:\n",
    "        json.dump(bbox_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved: class_mapping.json, CSVs, bboxes, summary\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# SAVE DATASET METADATA & CLASS MAPPING\n",
    "# Generate class mapping and summary statistics for training\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "# Save class mapping\n",
    "class_mapping = {i: class_name for i, class_name in enumerate(CLASS_NAMES)}\n",
    "with open('data/class_mapping.json', 'w') as f:\n",
    "    json.dump(class_mapping, f, indent=2)\n",
    "\n",
    "# Save metadata for each split\n",
    "train_df.to_csv('data/train_metadata.csv', index=False)\n",
    "val_df.to_csv('data/val_metadata.csv', index=False)\n",
    "test_df.to_csv('data/test_metadata.csv', index=False)\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary = {\n",
    "    'num_classes': len(CLASS_NAMES),\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'total_images': len(balanced_df),\n",
    "    'train_images': len(train_df),\n",
    "    'val_images': len(val_df),\n",
    "    'test_images': len(test_df),\n",
    "    'images_with_bboxes_train': int(train_df['Has_BBox'].sum()),\n",
    "    'images_with_bboxes_val': int(val_df['Has_BBox'].sum()),\n",
    "    'images_with_bboxes_test': int(test_df['Has_BBox'].sum()),\n",
    "    'date_processed': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'task': 'multi-class classification (15 classes) + segmentation'\n",
    "}\n",
    "\n",
    "with open('data/dataset_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Metadata saved: class_mapping.json, CSVs, bboxes, summary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

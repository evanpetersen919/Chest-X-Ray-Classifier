{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206cff5e",
   "metadata": {},
   "source": [
    "# NIH Chest X-Ray Dataset Preparation (Top 10 Diseases)\n",
    "\n",
    "## Overview\n",
    "This notebook prepares the NIH Chest X-Ray dataset for multi-label disease classification using the 10 most common disease categories. Rare diseases with insufficient training samples are excluded to improve model performance.\n",
    "\n",
    "## Key Features\n",
    "- **Patient-Aware Splitting**: Ensures no patient appears in multiple splits (train/val/test), preventing data leakage critical for medical AI validation\n",
    "- **Multi-Label Support**: Handles images with multiple disease classifications\n",
    "- **Flat Folder Structure**: Organizes images in a format compatible with multi-label learning\n",
    "- **Comprehensive Validation**: Verifies data integrity and distribution balance\n",
    "- **Production-Ready**: Implements best practices for medical imaging ML pipelines\n",
    "- **Top 10 Focus**: Uses only the 10 most common diseases (excludes rare cases with <300 samples)\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: NIH Clinical Center Chest X-Ray Dataset\n",
    "- **Total Images**: 112,120 frontal-view X-rays\n",
    "- **Diseases**: Top 10 most common (excludes Fibrosis, Hernia, Pleural_Thickening, Pneumonia, Pneumothorax)\n",
    "- **Sample Size**: 40,000 images (maintains statistical distribution)\n",
    "- **Splits**: 70% train, 15% validation, 15% test\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2535fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 112,120 chest X-ray image records\n",
      "Data location: C:\\xray_data (local SSD)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load dataset metadata containing image labels and patient information\n",
    "# Updated path: Data moved to C:\\xray_data for faster access (outside OneDrive)\n",
    "csv_path = r'C:\\xray_data\\archive (1)\\Data_Entry_2017.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Loaded {len(df):,} chest X-ray image records\")\n",
    "print(f\"Data location: C:\\\\xray_data (local SSD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc511d7",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "Load the complete dataset metadata from CSV files containing image labels and bounding box annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e04ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using TOP 10 Most Common Diseases:\n",
      "Disease Categories: 10\n",
      "  [ 0] Atelectasis               - 11,559 images\n",
      "  [ 1] Cardiomegaly              -  2,776 images\n",
      "  [ 2] Consolidation             -  4,667 images\n",
      "  [ 3] Effusion                  - 13,317 images\n",
      "  [ 4] Infiltration              - 19,894 images\n",
      "  [ 5] Mass                      -  5,782 images\n",
      "  [ 6] No Finding                - 60,361 images\n",
      "  [ 7] Nodule                    -  6,331 images\n",
      "  [ 8] Pleural_Thickening        -  3,385 images\n",
      "  [ 9] Pneumothorax              -  5,302 images\n",
      "\n",
      "Excluded diseases (rare cases):\n",
      "  Emphysema                 -  2,516 images\n",
      "  Edema                     -  2,303 images\n",
      "  Fibrosis                  -  1,686 images\n",
      "  Pneumonia                 -  1,431 images\n",
      "  Hernia                    -    227 images\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# PREPARE DISEASE CATEGORIES - TOP 10 ONLY\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Parse disease labels (pipe-separated for multi-label cases)\n",
    "all_diseases = []\n",
    "for labels in df['Finding Labels']:\n",
    "    diseases = labels.split('|')\n",
    "    all_diseases.extend(diseases)\n",
    "\n",
    "disease_counts = Counter(all_diseases)\n",
    "\n",
    "# Get top 10 most common diseases\n",
    "top_10_diseases = [disease for disease, count in disease_counts.most_common(10)]\n",
    "\n",
    "# Define class names (top 10 only, sorted for consistency)\n",
    "CLASS_NAMES = sorted(top_10_diseases)\n",
    "\n",
    "print(f\"\\nUsing TOP 10 Most Common Diseases:\")\n",
    "print(f\"Disease Categories: {len(CLASS_NAMES)}\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    count = disease_counts[class_name]\n",
    "    print(f\"  [{i:2}] {class_name:25} - {count:6,} images\")\n",
    "\n",
    "print(f\"\\nExcluded diseases (rare cases):\")\n",
    "excluded = [d for d in disease_counts.keys() if d not in CLASS_NAMES]\n",
    "for disease in sorted(excluded, key=lambda x: disease_counts[x], reverse=True):\n",
    "    count = disease_counts[disease]\n",
    "    print(f\"  {disease:25} - {count:6,} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b5f98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Composition:\n",
      "Total images: 112,120\n",
      "  Single disease: 91,324 (81.5%)\n",
      "  Multiple diseases: 20,796 (18.5%)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# PREPARE MULTI-LABEL DATA\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Preserve all data (no filtering)\n",
    "multilabel_df = df.copy()\n",
    "\n",
    "# Analyze multi-label distribution\n",
    "has_multiple = df['Finding Labels'].str.contains('|', regex=False)\n",
    "single_count = (~has_multiple).sum()\n",
    "multi_count = has_multiple.sum()\n",
    "\n",
    "print(f\"\\nDataset Composition:\")\n",
    "print(f\"Total images: {len(df):,}\")\n",
    "print(f\"  Single disease: {single_count:,} ({single_count/len(df)*100:.1f}%)\")\n",
    "print(f\"  Multiple diseases: {multi_count:,} ({multi_count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca114486",
   "metadata": {},
   "source": [
    "## 2. Dataset Statistics & Multi-Label Analysis\n",
    "Analyze the complete dataset to understand disease distribution and multi-label characteristics. This informs our approach to handling cases where patients have multiple conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfd4ae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmentation Data:\n",
      "  Total annotations: 984\n",
      "  Images with bounding boxes: 880\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD SEGMENTATION DATA\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "bbox_path = r'C:\\xray_data\\archive (1)\\BBox_List_2017.csv'\n",
    "bbox_df = pd.read_csv(bbox_path)\n",
    "\n",
    "# Flag images with bounding box annotations\n",
    "multilabel_df['Has_BBox'] = multilabel_df['Image Index'].isin(bbox_df['Image Index'])\n",
    "\n",
    "print(f\"\\nSegmentation Data:\")\n",
    "print(f\"  Total annotations: {len(bbox_df):,}\")\n",
    "print(f\"  Images with bounding boxes: {multilabel_df['Has_BBox'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b28994f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling Results:\n",
      "  Total: 40,000 images\n",
      "  Single disease: 32,651\n",
      "  Multiple diseases: 7,349\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# SAMPLE DATASET\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Configuration\n",
    "MAX_TOTAL_IMAGES = 40000  # Results in ~28k train, ~6k val, ~6k test\n",
    "\n",
    "# Random sampling with reproducibility\n",
    "if len(multilabel_df) > MAX_TOTAL_IMAGES:\n",
    "    sampled_df = resample(multilabel_df, n_samples=MAX_TOTAL_IMAGES, random_state=42)\n",
    "else:\n",
    "    sampled_df = multilabel_df\n",
    "\n",
    "# Shuffle for randomness\n",
    "sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nSampling Results:\")\n",
    "print(f\"  Total: {len(sampled_df):,} images\")\n",
    "print(f\"  Single disease: {(~sampled_df['Finding Labels'].str.contains('|', regex=False)).sum():,}\")\n",
    "print(f\"  Multiple diseases: {sampled_df['Finding Labels'].str.contains('|', regex=False).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9c5bc",
   "metadata": {},
   "source": [
    "## 3. Dataset Sampling\n",
    "Select a representative subset of 40,000 images to balance computational efficiency with statistical validity. This subset maintains the original distribution of disease prevalence and multi-label characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "105ab9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 28,153 images (221 with bboxes)\n",
      "Val:   5,887 images (61 with bboxes)\n",
      "Test:  5,960 images (35 with bboxes)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# PATIENT-AWARE TRAIN/VALIDATION/TEST SPLITTING (NO DATA LEAKAGE)\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract patient IDs (assumes first part of 'Image Index' is patient ID)\n",
    "sampled_df['PatientID'] = sampled_df['Image Index'].str.split('_').str[0]\n",
    "\n",
    "# Get unique patient IDs\n",
    "unique_patients = sampled_df['PatientID'].unique()\n",
    "\n",
    "# Split patient IDs into train/val/test\n",
    "train_patients, temp_patients = train_test_split(unique_patients, test_size=0.3, random_state=42)\n",
    "val_patients, test_patients = train_test_split(temp_patients, test_size=0.5, random_state=42)\n",
    "\n",
    "# Assign images to splits based on patient ID\n",
    "train_df = sampled_df[sampled_df['PatientID'].isin(train_patients)].reset_index(drop=True)\n",
    "val_df = sampled_df[sampled_df['PatientID'].isin(val_patients)].reset_index(drop=True)\n",
    "test_df = sampled_df[sampled_df['PatientID'].isin(test_patients)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df):,} images ({train_df['Has_BBox'].sum():,} with bboxes)\")\n",
    "print(f\"Val:   {len(val_df):,} images ({val_df['Has_BBox'].sum():,} with bboxes)\")\n",
    "print(f\"Test:  {len(test_df):,} images ({test_df['Has_BBox'].sum():,} with bboxes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77c554",
   "metadata": {},
   "source": [
    "## 4. Patient-Aware Data Splitting\n",
    "**Critical for Medical AI**: Split data by patient ID rather than by individual images. This prevents data leakage where multiple images from the same patient could appear in both training and test sets, which would artificially inflate model performance and compromise validation integrity.\n",
    "\n",
    "This approach ensures:\n",
    "- No patient overlap between train/validation/test sets\n",
    "- Realistic evaluation of model generalization to new patients\n",
    "- Compliance with best practices for medical ML research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5754952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Engineering Complete:\n",
      "  Created 10 binary disease indicators per image\n",
      "\n",
      "  Created 10 binary disease indicators per image\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CREATE BINARY DISEASE COLUMNS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Generate binary features for all splits\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    for disease in CLASS_NAMES:\n",
    "        df[disease] = df['Finding Labels'].str.contains(disease, regex=False).astype(int)\n",
    "\n",
    "print(f\"\\nFeature Engineering Complete:\")\n",
    "print(f\"  Created {len(CLASS_NAMES)} binary disease indicators per image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd13a7d",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "Create binary indicator columns for each disease to enable multi-label classification. Each image gets a 15-dimensional binary vector indicating presence/absence of each disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3dd2996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory Structure:\n",
      "  ✓ C:\\xray_data\\data\\train  (all training images)\n",
      "  ✓ C:\\xray_data\\data\\val    (all validation images)\n",
      "  ✓ C:\\xray_data\\data\\test   (all test images)\n",
      "\n",
      "Note: Labels stored in CSV metadata (multi-label compatible)\n",
      "Location: C:\\xray_data (local SSD, outside OneDrive)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CREATE FOLDER STRUCTURE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Use local C: drive location (outside OneDrive for faster access)\n",
    "base_dir = Path(r'C:\\xray_data\\data')\n",
    "\n",
    "# Create top-level split directories\n",
    "# data/train/, data/val/, data/test/ (no disease-specific subfolders)\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = base_dir / split\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nDirectory Structure:\")\n",
    "print(f\"  ✓ {base_dir / 'train'}  (all training images)\")\n",
    "print(f\"  ✓ {base_dir / 'val'}    (all validation images)\")\n",
    "print(f\"  ✓ {base_dir / 'test'}   (all test images)\")\n",
    "print(f\"\\nNote: Labels stored in CSV metadata (multi-label compatible)\")\n",
    "print(f\"Location: C:\\\\xray_data (local SSD, outside OneDrive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1b0e45",
   "metadata": {},
   "source": [
    "## 6. File Organization\n",
    "Organize images into a flat folder structure compatible with multi-label classification:\n",
    "- `data/train/` - Training images\n",
    "- `data/val/` - Validation images  \n",
    "- `data/test/` - Test images\n",
    "\n",
    "Labels are stored in CSV metadata files rather than folder structure, as images can have multiple disease labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54371e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already organized: 23593 images found\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# ORGANIZE FILES (PARALLEL PROCESSING)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from functools import lru_cache\n",
    "import glob\n",
    "\n",
    "# Check if data is already organized\n",
    "if (base_dir / 'train').exists() and len(list((base_dir / 'train').rglob('*.png'))) > 0:\n",
    "    print(f\"Data already organized: {len(list((base_dir / 'train').rglob('*.png')))} images found\")\n",
    "else:\n",
    "    # Cache the find_image results to avoid repeated glob searches\n",
    "    @lru_cache(maxsize=None)\n",
    "    def find_image_cached(image_name):\n",
    "        \"\"\"Find which folder contains a specific image (cached)\"\"\"\n",
    "        # Updated path for local C: drive\n",
    "        pattern = f\"C:/xray_data/archive (1)/images_*/images/{image_name}\"\n",
    "        matches = glob.glob(pattern)\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "        return None\n",
    "    \n",
    "    def copy_single_image(args):\n",
    "        \"\"\"Copy a single image (for parallel processing)\"\"\"\n",
    "        image_name, split_name = args\n",
    "        \n",
    "        # Find source image\n",
    "        source_path = find_image_cached(image_name)\n",
    "        if source_path is None:\n",
    "            return f\"Warning: Could not find {image_name}\"\n",
    "        \n",
    "        # FLAT STRUCTURE: All images directly in split folder\n",
    "        dest_path = base_dir / split_name / image_name\n",
    "        try:\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            return None  # Success\n",
    "        except Exception as e:\n",
    "            return f\"Error copying {image_name}: {e}\"\n",
    "    \n",
    "    def copy_images_to_folders_parallel(dataframe, split_name, max_workers=8):\n",
    "        \"\"\"Copy images using parallel processing for speed\"\"\"\n",
    "        # FLAT STRUCTURE: No class subfolders needed\n",
    "        args_list = [\n",
    "            (row['Image Index'], split_name)\n",
    "            for _, row in dataframe.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Use ThreadPoolExecutor for I/O-bound file copying\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Process files in parallel with progress bar\n",
    "            results = list(tqdm(\n",
    "                executor.map(copy_single_image, args_list),\n",
    "                total=len(args_list),\n",
    "                desc=f\"{split_name}\"\n",
    "            ))\n",
    "            # Collect errors\n",
    "            errors = [r for r in results if r is not None]\n",
    "        \n",
    "        return len(dataframe) - len(errors), len(errors)\n",
    "    \n",
    "    # Copy all splits with parallel processing\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_success, train_errors = copy_images_to_folders_parallel(train_df, 'train', max_workers=8)\n",
    "    val_success, val_errors = copy_images_to_folders_parallel(val_df, 'val', max_workers=8)\n",
    "    test_success, test_errors = copy_images_to_folders_parallel(test_df, 'test', max_workers=8)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    total_copied = train_success + val_success + test_success\n",
    "    \n",
    "    print(f\"\\nOrganized {total_copied:,} images in {elapsed:.1f}s ({total_copied/elapsed:.0f} img/s)\")\n",
    "    if train_errors + val_errors + test_errors > 0:\n",
    "        print(f\"Errors: {train_errors + val_errors + test_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcea86c",
   "metadata": {},
   "source": [
    "## 7. Parallel Image Processing\n",
    "Copy ~40,000 images to organized folders using multi-threaded processing for efficiency. Implementation uses Python's `concurrent.futures` for parallel I/O operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d769dc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATING DATASET ORGANIZATION\n",
      "================================================================================\n",
      "\n",
      "TRAIN Split:\n",
      "  Expected: 23,593 images\n",
      "  Actual:   23,593 images\n",
      "  Status: PASSED\n",
      "\n",
      "VAL Split:\n",
      "  Expected: 4,969 images\n",
      "  Actual:   4,969 images\n",
      "  Status: PASSED\n",
      "\n",
      "TEST Split:\n",
      "  Expected: 5,037 images\n",
      "  Actual:   5,037 images\n",
      "  Status: PASSED\n",
      "\n",
      "================================================================================\n",
      "ALL VALIDATIONS PASSED - Dataset ready for training\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DATA VALIDATION\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VALIDATING DATASET ORGANIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "splits_to_validate = ['train', 'val', 'test']\n",
    "validation_results = {}\n",
    "\n",
    "for split_name in splits_to_validate:\n",
    "    print(f\"\\n{split_name.upper()} Split:\")\n",
    "    \n",
    "    # Get expected images from CSV\n",
    "    if split_name == 'train':\n",
    "        expected_df = train_df\n",
    "    elif split_name == 'val':\n",
    "        expected_df = val_df\n",
    "    else:\n",
    "        expected_df = test_df\n",
    "    \n",
    "    expected_images = set(expected_df['Image Index'].tolist())\n",
    "    expected_count = len(expected_images)\n",
    "    \n",
    "    # Get actual images from folder\n",
    "    split_folder = base_dir / split_name\n",
    "    if not split_folder.exists():\n",
    "        print(f\"ERROR: Folder does not exist: {split_folder}\")\n",
    "        validation_results[split_name] = {'status': 'FAILED', 'reason': 'folder_missing'}\n",
    "        continue\n",
    "    \n",
    "    actual_images = set([f.name for f in split_folder.glob('*.png')])\n",
    "    actual_count = len(actual_images)\n",
    "    \n",
    "    print(f\"  Expected: {expected_count:,} images\")\n",
    "    print(f\"  Actual:   {actual_count:,} images\")\n",
    "    \n",
    "    if expected_count != actual_count:\n",
    "        print(f\"  WARNING: Count mismatch\")\n",
    "        missing = expected_images - actual_images\n",
    "        extra = actual_images - expected_images\n",
    "        if missing:\n",
    "            print(f\"  Missing {len(missing)} images\")\n",
    "        if extra:\n",
    "            print(f\"  Extra {len(extra)} images\")\n",
    "        validation_results[split_name] = {'status': 'WARNING', 'reason': 'count_mismatch'}\n",
    "    else:\n",
    "        missing = expected_images - actual_images\n",
    "        if missing:\n",
    "            print(f\"  ERROR: {len(missing)} expected images missing\")\n",
    "            validation_results[split_name] = {'status': 'FAILED', 'reason': 'missing_images'}\n",
    "        else:\n",
    "            extra = actual_images - expected_images\n",
    "            if extra:\n",
    "                print(f\"  WARNING: {len(extra)} unexpected extra images\")\n",
    "                validation_results[split_name] = {'status': 'WARNING', 'reason': 'extra_images'}\n",
    "            else:\n",
    "                print(f\"  Status: PASSED\")\n",
    "                validation_results[split_name] = {'status': 'PASSED'}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "all_passed = all(v['status'] == 'PASSED' for v in validation_results.values())\n",
    "if all_passed:\n",
    "    print(\"ALL VALIDATIONS PASSED - Dataset ready for training\")\n",
    "else:\n",
    "    print(\"VALIDATION WARNINGS/ERRORS - Review above for details\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973158c0",
   "metadata": {},
   "source": [
    "## 8. Data Validation\n",
    "Comprehensive validation to ensure data integrity:\n",
    "- Verify all expected images are present in organized folders\n",
    "- Check for missing or extra files\n",
    "- Validate image counts match metadata\n",
    "- Confirm successful file organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "585d97ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLASS DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Disease Distribution Across Splits:\n",
      "========================================================================================================================\n",
      "Disease                      Train      Val     Test    Total   Train%     Val%    Test%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "No Finding                   15272     3176     3225    21673    70.5%    14.7%    14.9%\n",
      "Infiltration                  5022      934     1049     7005    71.7%    13.3%    15.0%\n",
      "Effusion                      3266      677      668     4611    70.8%    14.7%    14.5%\n",
      "Atelectasis                   2871      627      653     4151    69.2%    15.1%    15.7%\n",
      "Nodule                        1564      355      302     2221    70.4%    16.0%    13.6%\n",
      "Mass                          1419      353      339     2111    67.2%    16.7%    16.1%\n",
      "Pneumothorax                  1215      320      282     1817    66.9%    17.6%    15.5%\n",
      "Consolidation                 1204      228      242     1674    71.9%    13.6%    14.5%\n",
      "Pleural_Thickening             854      214      160     1228    69.5%    17.4%    13.0%\n",
      "Cardiomegaly                   664      144      111      919    72.3%    15.7%    12.1%\n",
      "\n",
      "Average split percentages: Train=70.0%, Val=15.5%, Test=14.5%\n",
      "Target: Train=70%, Val=15%, Test=15%\n",
      "\n",
      "================================================================================\n",
      "MULTI-LABEL STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Train:\n",
      "  Total images: 28,153\n",
      "  Single disease:  23,040 (81.8%)\n",
      "  Multi-disease: 4,393 (15.6%)\n",
      "  Max diseases/image: 7\n",
      "  Avg diseases/image: 1.18\n",
      "\n",
      "Val:\n",
      "  Total images: 5,887\n",
      "  Single disease:  4,800 (81.5%)\n",
      "  Multi-disease: 940 (16.0%)\n",
      "  Max diseases/image: 6\n",
      "  Avg diseases/image: 1.19\n",
      "\n",
      "Test:\n",
      "  Total images: 5,960\n",
      "  Single disease:  4,876 (81.8%)\n",
      "  Multi-disease: 927 (15.6%)\n",
      "  Max diseases/image: 7\n",
      "  Avg diseases/image: 1.18\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CLASS DISTRIBUTION ANALYSIS\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze disease distribution across splits\n",
    "disease_stats = []\n",
    "\n",
    "for disease in CLASS_NAMES:\n",
    "    train_count = train_df[disease].sum()\n",
    "    val_count = val_df[disease].sum()\n",
    "    test_count = test_df[disease].sum()\n",
    "    total = train_count + val_count + test_count\n",
    "    \n",
    "    disease_stats.append({\n",
    "        'Disease': disease,\n",
    "        'Train': train_count,\n",
    "        'Val': val_count,\n",
    "        'Test': test_count,\n",
    "        'Total': total,\n",
    "        'Train%': 100 * train_count / total if total > 0 else 0,\n",
    "        'Val%': 100 * val_count / total if total > 0 else 0,\n",
    "        'Test%': 100 * test_count / total if total > 0 else 0\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "dist_df = pd.DataFrame(disease_stats)\n",
    "dist_df = dist_df.sort_values('Total', ascending=False)\n",
    "\n",
    "print(\"\\nDisease Distribution Across Splits:\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"{'Disease':<25} {'Train':>8} {'Val':>8} {'Test':>8} {'Total':>8} {'Train%':>8} {'Val%':>8} {'Test%':>8}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for _, row in dist_df.iterrows():\n",
    "    print(f\"{row['Disease']:<25} {row['Train']:>8.0f} {row['Val']:>8.0f} {row['Test']:>8.0f} \"\n",
    "          f\"{row['Total']:>8.0f} {row['Train%']:>7.1f}% {row['Val%']:>7.1f}% {row['Test%']:>7.1f}%\")\n",
    "\n",
    "# Check for balanced splits\n",
    "avg_train_pct = dist_df['Train%'].mean()\n",
    "avg_val_pct = dist_df['Val%'].mean()\n",
    "avg_test_pct = dist_df['Test%'].mean()\n",
    "\n",
    "print(f\"\\nAverage split percentages: Train={avg_train_pct:.1f}%, Val={avg_val_pct:.1f}%, Test={avg_test_pct:.1f}%\")\n",
    "print(f\"Target: Train=70%, Val=15%, Test=15%\")\n",
    "\n",
    "# Check if any disease has severely unbalanced split\n",
    "issues = []\n",
    "for _, row in dist_df.iterrows():\n",
    "    if abs(row['Train%'] - 70) > 10 or abs(row['Val%'] - 15) > 5 or abs(row['Test%'] - 15) > 5:\n",
    "        issues.append(f\"  {row['Disease']}: Train={row['Train%']:.1f}%, Val={row['Val%']:.1f}%, Test={row['Test%']:.1f}%\")\n",
    "\n",
    "if issues:\n",
    "    print(\"\\nWARNING: Some diseases have unbalanced splits (>10% deviation):\")\n",
    "    for issue in issues:\n",
    "        print(issue)\n",
    "\n",
    "# Multi-label statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTI-LABEL STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for split_name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    diseases_per_image = df[CLASS_NAMES].sum(axis=1)\n",
    "    \n",
    "    print(f\"\\n{split_name}:\")\n",
    "    print(f\"  Total images: {len(df):,}\")\n",
    "    print(f\"  Single disease:  {(diseases_per_image == 1).sum():,} ({100*(diseases_per_image == 1).sum()/len(df):.1f}%)\")\n",
    "    print(f\"  Multi-disease: {(diseases_per_image > 1).sum():,} ({100*(diseases_per_image > 1).sum()/len(df):.1f}%)\")\n",
    "    print(f\"  Max diseases/image: {diseases_per_image.max():.0f}\")\n",
    "    print(f\"  Avg diseases/image: {diseases_per_image.mean():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37b7c8",
   "metadata": {},
   "source": [
    "## 9. Class Distribution Analysis\n",
    "Analyze disease distribution across splits to ensure:\n",
    "- Balanced representation of all disease categories\n",
    "- Consistent split ratios (70/15/15) across diseases\n",
    "- Documentation of multi-label characteristics\n",
    "- Statistical validation of sampling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01655567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# SAVE SEGMENTATION ANNOTATIONS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "# Save bounding box annotations for each split\n",
    "for split_name, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "    # Get images with bounding boxes\n",
    "    images_with_bbox = split_df[split_df['Has_BBox']]['Image Index'].tolist()\n",
    "    \n",
    "    # Filter bbox data for this split\n",
    "    split_bbox = bbox_df[bbox_df['Image Index'].isin(images_with_bbox)]\n",
    "    \n",
    "    bbox_dict = {}\n",
    "    for _, row in split_bbox.iterrows():\n",
    "        img_name = row['Image Index']\n",
    "        if img_name not in bbox_dict:\n",
    "            bbox_dict[img_name] = []\n",
    "        bbox_dict[img_name].append({\n",
    "            'class': row['Finding Label'],\n",
    "            'x': row['Bbox [x'],\n",
    "            'y': row['y'],\n",
    "            'w': row['w'],\n",
    "            'h': row['h]']\n",
    "        })\n",
    "    \n",
    "    # Save to file (updated path for local C: drive)\n",
    "    output_path = f'C:/xray_data/data/{split_name}_bboxes.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(bbox_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e181c7",
   "metadata": {},
   "source": [
    "## 10. Export Metadata & Annotations\n",
    "Generate comprehensive metadata files for model training:\n",
    "- **Class mapping**: Disease ID to name mapping\n",
    "- **CSV metadata**: Train/val/test labels and patient IDs\n",
    "- **Bounding boxes**: Segmentation annotations (JSON format)\n",
    "- **Summary statistics**: Complete dataset documentation including patient-level statistics and validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7578a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata saved to C:\\xray_data\\data\\\n",
      "  - class_mapping.json\n",
      "  - train_metadata.csv, val_metadata.csv, test_metadata.csv\n",
      "  - train_bboxes.json, val_bboxes.json, test_bboxes.json\n",
      "  - dataset_summary.json\n",
      "\n",
      "Patient-aware splitting:\n",
      "  Train: 10,653 patients\n",
      "  Val:   2,283 patients\n",
      "  Test:  2,283 patients\n",
      "  No patient overlap between splits\n",
      "\n",
      "Multi-label classification (TOP 10 diseases):\n",
      "  Single disease: 32,651 images\n",
      "  Multi-disease: 7,349 images\n",
      "\n",
      "Folder structure: Flat (all images in C:\\xray_data\\data\\train\\ val\\ test\\)\n",
      "Labels: Stored in CSV files (multi-label compatible)\n",
      "Location: C:\\xray_data (local SSD, outside OneDrive)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# SAVE DATASET METADATA\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "# Save class mapping (updated path for local C: drive)\n",
    "class_mapping = {i: class_name for i, class_name in enumerate(CLASS_NAMES)}\n",
    "with open('C:/xray_data/data/class_mapping.json', 'w') as f:\n",
    "    json.dump(class_mapping, f, indent=2)\n",
    "\n",
    "# Save metadata for each split\n",
    "train_df.to_csv('C:/xray_data/data/train_metadata.csv', index=False)\n",
    "val_df.to_csv('C:/xray_data/data/val_metadata.csv', index=False)\n",
    "test_df.to_csv('C:/xray_data/data/test_metadata.csv', index=False)\n",
    "\n",
    "# Create comprehensive summary with patient-level and validation stats\n",
    "from collections import Counter\n",
    "\n",
    "# Extract patient IDs from each split\n",
    "train_patients = set(train_df['Image Index'].str[:8])\n",
    "val_patients = set(val_df['Image Index'].str[:8])\n",
    "test_patients = set(test_df['Image Index'].str[:8])\n",
    "\n",
    "# Count images per disease\n",
    "disease_counts = {}\n",
    "for disease in CLASS_NAMES:\n",
    "    disease_counts[disease] = {\n",
    "        'train': int(train_df[disease].sum()),\n",
    "        'val': int(val_df[disease].sum()),\n",
    "        'test': int(test_df[disease].sum()),\n",
    "        'total': int(train_df[disease].sum() + val_df[disease].sum() + test_df[disease].sum())\n",
    "    }\n",
    "\n",
    "summary = {\n",
    "    'num_classes': len(CLASS_NAMES),\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'total_images': len(sampled_df),\n",
    "    'train_images': len(train_df),\n",
    "    'val_images': len(val_df),\n",
    "    'test_images': len(test_df),\n",
    "    'images_with_bboxes_train': int(train_df['Has_BBox'].sum()),\n",
    "    'images_with_bboxes_val': int(val_df['Has_BBox'].sum()),\n",
    "    'images_with_bboxes_test': int(test_df['Has_BBox'].sum()),\n",
    "    'date_processed': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'task': 'multi-label classification (10 classes - top diseases only) + segmentation',\n",
    "    'multi_label': True,\n",
    "    'single_label_count': int((~sampled_df['Finding Labels'].str.contains('|', regex=False)).sum()),\n",
    "    'multi_label_count': int(sampled_df['Finding Labels'].str.contains('|', regex=False).sum()),\n",
    "    # Patient-level statistics (CRITICAL for medical AI)\n",
    "    'patient_splitting': {\n",
    "        'enabled': True,\n",
    "        'unique_patients_train': len(train_patients),\n",
    "        'unique_patients_val': len(val_patients),\n",
    "        'unique_patients_test': len(test_patients),\n",
    "        'unique_patients_total': len(train_patients) + len(val_patients) + len(test_patients),\n",
    "        'patient_overlap_train_val': len(train_patients & val_patients),\n",
    "        'patient_overlap_train_test': len(train_patients & test_patients),\n",
    "        'patient_overlap_val_test': len(val_patients & test_patients),\n",
    "        'avg_images_per_patient': len(sampled_df) / (len(train_patients) + len(val_patients) + len(test_patients))\n",
    "    },\n",
    "    # Validation results\n",
    "    'validation_status': validation_results,\n",
    "    'all_validations_passed': all(v['status'] == 'PASSED' for v in validation_results.values()),\n",
    "    # Disease distribution\n",
    "    'disease_distribution': disease_counts,\n",
    "    # Folder structure\n",
    "    'folder_structure': 'flat',  # All images directly in split folders (train/, val/, test/)\n",
    "    'labels_stored_in': 'CSV files only (multi-label compatible)',\n",
    "    'data_location': 'C:\\\\xray_data (local SSD, outside OneDrive for faster training)'\n",
    "}\n",
    "\n",
    "with open('C:/xray_data/data/dataset_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nMetadata saved to C:\\\\xray_data\\\\data\\\\\")\n",
    "print(\"  - class_mapping.json\")\n",
    "print(\"  - train_metadata.csv, val_metadata.csv, test_metadata.csv\")\n",
    "print(\"  - train_bboxes.json, val_bboxes.json, test_bboxes.json\")\n",
    "print(\"  - dataset_summary.json\")\n",
    "print(f\"\\nPatient-aware splitting:\")\n",
    "print(f\"  Train: {len(train_patients):,} patients\")\n",
    "print(f\"  Val:   {len(val_patients):,} patients\")\n",
    "print(f\"  Test:  {len(test_patients):,} patients\")\n",
    "print(f\"  No patient overlap between splits\")\n",
    "print(f\"\\nMulti-label classification (TOP 10 diseases):\")\n",
    "print(f\"  Single disease: {summary['single_label_count']:,} images\")\n",
    "print(f\"  Multi-disease: {summary['multi_label_count']:,} images\")\n",
    "print(f\"\\nFolder structure: Flat (all images in C:\\\\xray_data\\\\data\\\\train\\\\ val\\\\ test\\\\)\")\n",
    "print(f\"Labels: Stored in CSV files (multi-label compatible)\")\n",
    "print(f\"Location: C:\\\\xray_data (local SSD, outside OneDrive)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206cff5e",
   "metadata": {},
   "source": [
    "# NIH Chest X-Ray Dataset Preparation (Top 7 Diseases)\n",
    "\n",
    "## Overview\n",
    "This notebook prepares the NIH Chest X-Ray dataset for multi-label disease classification using the 7 most trainable disease categories. Diseases with insufficient training samples (<1,500 images) are excluded to improve model performance and avoid overfitting.\n",
    "\n",
    "## Key Features\n",
    "- **Patient-Aware Splitting**: Ensures no patient appears in multiple splits (train/val/test), preventing data leakage critical for medical AI validation\n",
    "- **Multi-Label Support**: Handles images with multiple disease classifications\n",
    "- **Flat Folder Structure**: Organizes images in a format compatible with multi-label learning\n",
    "- **Comprehensive Validation**: Verifies data integrity and distribution balance\n",
    "- **Production-Ready**: Implements best practices for medical imaging ML pipelines\n",
    "- **Top 7 Focus**: Uses only 7 diseases with adequate training data (excludes Cardiomegaly, Consolidation, Pleural_Thickening)\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: NIH Clinical Center Chest X-Ray Dataset\n",
    "- **Total Images**: 112,120 frontal-view X-rays\n",
    "- **Diseases**: Top 7 trainable diseases (excludes rare diseases with <1,500 samples)\n",
    "- **Sample Size**: 40,000 images (maintains statistical distribution)\n",
    "- **Splits**: 70% train, 15% validation, 15% test\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load dataset metadata containing image labels and patient information\n",
    "# Updated path: Data moved to C:\\xray_data for faster access (outside OneDrive)\n",
    "csv_path = r'C:\\xray_data\\archive (1)\\Data_Entry_2017.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Loaded {len(df):,} chest X-ray image records\")\n",
    "print(f\"Data location: C:\\\\xray_data (local SSD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc511d7",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "Load the complete dataset metadata from CSV files containing image labels and bounding box annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# PREPARE DISEASE CATEGORIES - TOP 7 ONLY\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Parse disease labels (pipe-separated for multi-label cases)\n",
    "all_diseases = []\n",
    "for labels in df['Finding Labels']:\n",
    "    diseases = labels.split('|')\n",
    "    all_diseases.extend(diseases)\n",
    "\n",
    "disease_counts = Counter(all_diseases)\n",
    "\n",
    "# Define the 7 trainable diseases (manually selected for adequate training data)\n",
    "# Excluded: Cardiomegaly (664 samples), Consolidation (1,204 samples), Pleural_Thickening (160 samples)\n",
    "TRAINABLE_DISEASES = [\n",
    "    'Atelectasis',\n",
    "    'Effusion', \n",
    "    'Infiltration',\n",
    "    'Mass',\n",
    "    'No Finding',\n",
    "    'Nodule',\n",
    "    'Pneumothorax'\n",
    "]\n",
    "\n",
    "# Define class names (sorted for consistency)\n",
    "CLASS_NAMES = sorted(TRAINABLE_DISEASES)\n",
    "\n",
    "print(f\"\\nUsing TOP 7 Trainable Diseases (Quality over Quantity):\")\n",
    "print(f\"Disease Categories: {len(CLASS_NAMES)}\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    count = disease_counts[class_name]\n",
    "    print(f\"  [{i:2}] {class_name:25} - {count:6,} images\")\n",
    "\n",
    "print(f\"\\nExcluded diseases (insufficient training data):\")\n",
    "excluded = [d for d in disease_counts.keys() if d not in CLASS_NAMES]\n",
    "for disease in sorted(excluded, key=lambda x: disease_counts[x], reverse=True):\n",
    "    count = disease_counts[disease]\n",
    "    reason = \"\"\n",
    "    if disease in ['Cardiomegaly', 'Consolidation', 'Pleural_Thickening']:\n",
    "        reason = \" (< 1,500 samples - insufficient data)\"\n",
    "    print(f\"  {disease:25} - {count:6,} images{reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# PREPARE MULTI-LABEL DATA\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Preserve all data (no filtering)\n",
    "multilabel_df = df.copy()\n",
    "\n",
    "# Analyze multi-label distribution\n",
    "has_multiple = df['Finding Labels'].str.contains('|', regex=False)\n",
    "single_count = (~has_multiple).sum()\n",
    "multi_count = has_multiple.sum()\n",
    "\n",
    "print(f\"\\nDataset Composition:\")\n",
    "print(f\"Total images: {len(df):,}\")\n",
    "print(f\"  Single disease: {single_count:,} ({single_count/len(df)*100:.1f}%)\")\n",
    "print(f\"  Multiple diseases: {multi_count:,} ({multi_count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca114486",
   "metadata": {},
   "source": [
    "## 2. Dataset Statistics & Multi-Label Analysis\n",
    "Analyze the complete dataset to understand disease distribution and multi-label characteristics. This informs our approach to handling cases where patients have multiple conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LOAD SEGMENTATION DATA (Future Project)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "bbox_path = r'C:\\xray_data\\archive (1)\\BBox_List_2017.csv'\n",
    "bbox_df = pd.read_csv(bbox_path)\n",
    "\n",
    "# Flag images with bounding box annotations\n",
    "multilabel_df['Has_BBox'] = multilabel_df['Image Index'].isin(bbox_df['Image Index'])\n",
    "\n",
    "print(f\"\\nSegmentation Data:\")\n",
    "print(f\"  Total annotations: {len(bbox_df):,}\")\n",
    "print(f\"  Images with bounding boxes: {multilabel_df['Has_BBox'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28994f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# SAMPLE DATASET\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Configuration\n",
    "MAX_TOTAL_IMAGES = 40000  # Results in ~28k train, ~6k val, ~6k test\n",
    "\n",
    "# Random sampling with reproducibility\n",
    "if len(multilabel_df) > MAX_TOTAL_IMAGES:\n",
    "    sampled_df = resample(multilabel_df, n_samples=MAX_TOTAL_IMAGES, random_state=42)\n",
    "else:\n",
    "    sampled_df = multilabel_df\n",
    "\n",
    "# Shuffle for randomness\n",
    "sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nSampling Results:\")\n",
    "print(f\"  Total: {len(sampled_df):,} images\")\n",
    "print(f\"  Single disease: {(~sampled_df['Finding Labels'].str.contains('|', regex=False)).sum():,}\")\n",
    "print(f\"  Multiple diseases: {sampled_df['Finding Labels'].str.contains('|', regex=False).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9c5bc",
   "metadata": {},
   "source": [
    "## 3. Dataset Sampling\n",
    "Select a representative subset of 40,000 images to balance computational efficiency with statistical validity. This subset maintains the original distribution of disease prevalence and multi-label characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ab9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# PATIENT-AWARE TRAIN/VALIDATION/TEST SPLITTING (NO DATA LEAKAGE)\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract patient IDs (assumes first part of 'Image Index' is patient ID)\n",
    "sampled_df['PatientID'] = sampled_df['Image Index'].str.split('_').str[0]\n",
    "\n",
    "# Get unique patient IDs\n",
    "unique_patients = sampled_df['PatientID'].unique()\n",
    "\n",
    "# Split patient IDs into train/val/test\n",
    "train_patients, temp_patients = train_test_split(unique_patients, test_size=0.3, random_state=42)\n",
    "val_patients, test_patients = train_test_split(temp_patients, test_size=0.5, random_state=42)\n",
    "\n",
    "# Assign images to splits based on patient ID\n",
    "train_df = sampled_df[sampled_df['PatientID'].isin(train_patients)].reset_index(drop=True)\n",
    "val_df = sampled_df[sampled_df['PatientID'].isin(val_patients)].reset_index(drop=True)\n",
    "test_df = sampled_df[sampled_df['PatientID'].isin(test_patients)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df):,} images ({train_df['Has_BBox'].sum():,} with bboxes)\")\n",
    "print(f\"Val:   {len(val_df):,} images ({val_df['Has_BBox'].sum():,} with bboxes)\")\n",
    "print(f\"Test:  {len(test_df):,} images ({test_df['Has_BBox'].sum():,} with bboxes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77c554",
   "metadata": {},
   "source": [
    "## 4. Patient-Aware Data Splitting\n",
    "**Critical for Medical AI**: Split data by patient ID rather than by individual images. This prevents data leakage where multiple images from the same patient could appear in both training and test sets, which would artificially inflate model performance and compromise validation integrity.\n",
    "\n",
    "This approach ensures:\n",
    "- No patient overlap between train/validation/test sets\n",
    "- Realistic evaluation of model generalization to new patients\n",
    "- Compliance with best practices for medical ML research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5754952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CREATE BINARY DISEASE COLUMNS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Generate binary features for all splits\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    for disease in CLASS_NAMES:\n",
    "        df[disease] = df['Finding Labels'].str.contains(disease, regex=False).astype(int)\n",
    "\n",
    "print(f\"\\nFeature Engineering Complete:\")\n",
    "print(f\"  Created {len(CLASS_NAMES)} binary disease indicators per image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd13a7d",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "Create binary indicator columns for each disease to enable multi-label classification. Each image gets a 15-dimensional binary vector indicating presence/absence of each disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CREATE FOLDER STRUCTURE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Use local C: drive location (outside OneDrive for faster access)\n",
    "base_dir = Path(r'C:\\xray_data\\data')\n",
    "\n",
    "# Create top-level split directories\n",
    "# data/train/, data/val/, data/test/ (no disease-specific subfolders)\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = base_dir / split\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nDirectory Structure:\")\n",
    "print(f\"  ✓ {base_dir / 'train'}  (all training images)\")\n",
    "print(f\"  ✓ {base_dir / 'val'}    (all validation images)\")\n",
    "print(f\"  ✓ {base_dir / 'test'}   (all test images)\")\n",
    "print(f\"\\nNote: Labels stored in CSV metadata (multi-label compatible)\")\n",
    "print(f\"Location: C:\\\\xray_data (local SSD, outside OneDrive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1b0e45",
   "metadata": {},
   "source": [
    "## 6. File Organization\n",
    "Organize images into a flat folder structure compatible with multi-label classification:\n",
    "- `data/train/` - Training images\n",
    "- `data/val/` - Validation images  \n",
    "- `data/test/` - Test images\n",
    "\n",
    "Labels are stored in CSV metadata files rather than folder structure, as images can have multiple disease labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54371e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# ORGANIZE FILES (PARALLEL PROCESSING)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from functools import lru_cache\n",
    "import glob\n",
    "\n",
    "# Check if data is already organized\n",
    "if (base_dir / 'train').exists() and len(list((base_dir / 'train').rglob('*.png'))) > 0:\n",
    "    print(f\"Data already organized: {len(list((base_dir / 'train').rglob('*.png')))} images found\")\n",
    "else:\n",
    "    # Cache the find_image results to avoid repeated glob searches\n",
    "    @lru_cache(maxsize=None)\n",
    "    def find_image_cached(image_name):\n",
    "        \"\"\"Find which folder contains a specific image (cached)\"\"\"\n",
    "        # Updated path for local C: drive\n",
    "        pattern = f\"C:/xray_data/archive (1)/images_*/images/{image_name}\"\n",
    "        matches = glob.glob(pattern)\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "        return None\n",
    "    \n",
    "    def copy_single_image(args):\n",
    "        \"\"\"Copy a single image (for parallel processing)\"\"\"\n",
    "        image_name, split_name = args\n",
    "        \n",
    "        # Find source image\n",
    "        source_path = find_image_cached(image_name)\n",
    "        if source_path is None:\n",
    "            return f\"Warning: Could not find {image_name}\"\n",
    "        \n",
    "        # FLAT STRUCTURE: All images directly in split folder\n",
    "        dest_path = base_dir / split_name / image_name\n",
    "        try:\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            return None  # Success\n",
    "        except Exception as e:\n",
    "            return f\"Error copying {image_name}: {e}\"\n",
    "    \n",
    "    def copy_images_to_folders_parallel(dataframe, split_name, max_workers=8):\n",
    "        \"\"\"Copy images using parallel processing for speed\"\"\"\n",
    "        # FLAT STRUCTURE: No class subfolders needed\n",
    "        args_list = [\n",
    "            (row['Image Index'], split_name)\n",
    "            for _, row in dataframe.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Use ThreadPoolExecutor for I/O-bound file copying\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Process files in parallel with progress bar\n",
    "            results = list(tqdm(\n",
    "                executor.map(copy_single_image, args_list),\n",
    "                total=len(args_list),\n",
    "                desc=f\"{split_name}\"\n",
    "            ))\n",
    "            # Collect errors\n",
    "            errors = [r for r in results if r is not None]\n",
    "        \n",
    "        return len(dataframe) - len(errors), len(errors)\n",
    "    \n",
    "    # Copy all splits with parallel processing\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_success, train_errors = copy_images_to_folders_parallel(train_df, 'train', max_workers=8)\n",
    "    val_success, val_errors = copy_images_to_folders_parallel(val_df, 'val', max_workers=8)\n",
    "    test_success, test_errors = copy_images_to_folders_parallel(test_df, 'test', max_workers=8)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    total_copied = train_success + val_success + test_success\n",
    "    \n",
    "    print(f\"\\nOrganized {total_copied:,} images in {elapsed:.1f}s ({total_copied/elapsed:.0f} img/s)\")\n",
    "    if train_errors + val_errors + test_errors > 0:\n",
    "        print(f\"Errors: {train_errors + val_errors + test_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcea86c",
   "metadata": {},
   "source": [
    "## 7. Parallel Image Processing\n",
    "Copy ~40,000 images to organized folders using multi-threaded processing for efficiency. Implementation uses Python's `concurrent.futures` for parallel I/O operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DATA VALIDATION\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VALIDATING DATASET ORGANIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "splits_to_validate = ['train', 'val', 'test']\n",
    "validation_results = {}\n",
    "\n",
    "for split_name in splits_to_validate:\n",
    "    print(f\"\\n{split_name.upper()} Split:\")\n",
    "    \n",
    "    # Get expected images from CSV\n",
    "    if split_name == 'train':\n",
    "        expected_df = train_df\n",
    "    elif split_name == 'val':\n",
    "        expected_df = val_df\n",
    "    else:\n",
    "        expected_df = test_df\n",
    "    \n",
    "    expected_images = set(expected_df['Image Index'].tolist())\n",
    "    expected_count = len(expected_images)\n",
    "    \n",
    "    # Get actual images from folder\n",
    "    split_folder = base_dir / split_name\n",
    "    if not split_folder.exists():\n",
    "        print(f\"ERROR: Folder does not exist: {split_folder}\")\n",
    "        validation_results[split_name] = {'status': 'FAILED', 'reason': 'folder_missing'}\n",
    "        continue\n",
    "    \n",
    "    actual_images = set([f.name for f in split_folder.glob('*.png')])\n",
    "    actual_count = len(actual_images)\n",
    "    \n",
    "    print(f\"  Expected: {expected_count:,} images\")\n",
    "    print(f\"  Actual:   {actual_count:,} images\")\n",
    "    \n",
    "    if expected_count != actual_count:\n",
    "        print(f\"  WARNING: Count mismatch\")\n",
    "        missing = expected_images - actual_images\n",
    "        extra = actual_images - expected_images\n",
    "        if missing:\n",
    "            print(f\"  Missing {len(missing)} images\")\n",
    "        if extra:\n",
    "            print(f\"  Extra {len(extra)} images\")\n",
    "        validation_results[split_name] = {'status': 'WARNING', 'reason': 'count_mismatch'}\n",
    "    else:\n",
    "        missing = expected_images - actual_images\n",
    "        if missing:\n",
    "            print(f\"  ERROR: {len(missing)} expected images missing\")\n",
    "            validation_results[split_name] = {'status': 'FAILED', 'reason': 'missing_images'}\n",
    "        else:\n",
    "            extra = actual_images - expected_images\n",
    "            if extra:\n",
    "                print(f\"  WARNING: {len(extra)} unexpected extra images\")\n",
    "                validation_results[split_name] = {'status': 'WARNING', 'reason': 'extra_images'}\n",
    "            else:\n",
    "                print(f\"  Status: PASSED\")\n",
    "                validation_results[split_name] = {'status': 'PASSED'}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "all_passed = all(v['status'] == 'PASSED' for v in validation_results.values())\n",
    "if all_passed:\n",
    "    print(\"ALL VALIDATIONS PASSED - Dataset ready for training\")\n",
    "else:\n",
    "    print(\"VALIDATION WARNINGS/ERRORS - Review above for details\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973158c0",
   "metadata": {},
   "source": [
    "## 8. Data Validation\n",
    "Comprehensive validation to ensure data integrity:\n",
    "- Verify all expected images are present in organized folders\n",
    "- Check for missing or extra files\n",
    "- Validate image counts match metadata\n",
    "- Confirm successful file organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d97ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CLASS DISTRIBUTION ANALYSIS\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze disease distribution across splits\n",
    "disease_stats = []\n",
    "\n",
    "for disease in CLASS_NAMES:\n",
    "    train_count = train_df[disease].sum()\n",
    "    val_count = val_df[disease].sum()\n",
    "    test_count = test_df[disease].sum()\n",
    "    total = train_count + val_count + test_count\n",
    "    \n",
    "    disease_stats.append({\n",
    "        'Disease': disease,\n",
    "        'Train': train_count,\n",
    "        'Val': val_count,\n",
    "        'Test': test_count,\n",
    "        'Total': total,\n",
    "        'Train%': 100 * train_count / total if total > 0 else 0,\n",
    "        'Val%': 100 * val_count / total if total > 0 else 0,\n",
    "        'Test%': 100 * test_count / total if total > 0 else 0\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "dist_df = pd.DataFrame(disease_stats)\n",
    "dist_df = dist_df.sort_values('Total', ascending=False)\n",
    "\n",
    "print(\"\\nDisease Distribution Across Splits:\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"{'Disease':<25} {'Train':>8} {'Val':>8} {'Test':>8} {'Total':>8} {'Train%':>8} {'Val%':>8} {'Test%':>8}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for _, row in dist_df.iterrows():\n",
    "    print(f\"{row['Disease']:<25} {row['Train']:>8.0f} {row['Val']:>8.0f} {row['Test']:>8.0f} \"\n",
    "          f\"{row['Total']:>8.0f} {row['Train%']:>7.1f}% {row['Val%']:>7.1f}% {row['Test%']:>7.1f}%\")\n",
    "\n",
    "# Check for balanced splits\n",
    "avg_train_pct = dist_df['Train%'].mean()\n",
    "avg_val_pct = dist_df['Val%'].mean()\n",
    "avg_test_pct = dist_df['Test%'].mean()\n",
    "\n",
    "print(f\"\\nAverage split percentages: Train={avg_train_pct:.1f}%, Val={avg_val_pct:.1f}%, Test={avg_test_pct:.1f}%\")\n",
    "print(f\"Target: Train=70%, Val=15%, Test=15%\")\n",
    "\n",
    "# Check if any disease has severely unbalanced split\n",
    "issues = []\n",
    "for _, row in dist_df.iterrows():\n",
    "    if abs(row['Train%'] - 70) > 10 or abs(row['Val%'] - 15) > 5 or abs(row['Test%'] - 15) > 5:\n",
    "        issues.append(f\"  {row['Disease']}: Train={row['Train%']:.1f}%, Val={row['Val%']:.1f}%, Test={row['Test%']:.1f}%\")\n",
    "\n",
    "if issues:\n",
    "    print(\"\\nWARNING: Some diseases have unbalanced splits (>10% deviation):\")\n",
    "    for issue in issues:\n",
    "        print(issue)\n",
    "\n",
    "# Multi-label statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTI-LABEL STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for split_name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    diseases_per_image = df[CLASS_NAMES].sum(axis=1)\n",
    "    \n",
    "    print(f\"\\n{split_name}:\")\n",
    "    print(f\"  Total images: {len(df):,}\")\n",
    "    print(f\"  Single disease:  {(diseases_per_image == 1).sum():,} ({100*(diseases_per_image == 1).sum()/len(df):.1f}%)\")\n",
    "    print(f\"  Multi-disease: {(diseases_per_image > 1).sum():,} ({100*(diseases_per_image > 1).sum()/len(df):.1f}%)\")\n",
    "    print(f\"  Max diseases/image: {diseases_per_image.max():.0f}\")\n",
    "    print(f\"  Avg diseases/image: {diseases_per_image.mean():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37b7c8",
   "metadata": {},
   "source": [
    "## 9. Class Distribution Analysis\n",
    "Analyze disease distribution across splits to ensure:\n",
    "- Balanced representation of all disease categories\n",
    "- Consistent split ratios (70/15/15) across diseases\n",
    "- Documentation of multi-label characteristics\n",
    "- Statistical validation of sampling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01655567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# SAVE SEGMENTATION ANNOTATIONS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "# Save bounding box annotations for each split\n",
    "for split_name, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "    # Get images with bounding boxes\n",
    "    images_with_bbox = split_df[split_df['Has_BBox']]['Image Index'].tolist()\n",
    "    \n",
    "    # Filter bbox data for this split\n",
    "    split_bbox = bbox_df[bbox_df['Image Index'].isin(images_with_bbox)]\n",
    "    \n",
    "    bbox_dict = {}\n",
    "    for _, row in split_bbox.iterrows():\n",
    "        img_name = row['Image Index']\n",
    "        if img_name not in bbox_dict:\n",
    "            bbox_dict[img_name] = []\n",
    "        bbox_dict[img_name].append({\n",
    "            'class': row['Finding Label'],\n",
    "            'x': row['Bbox [x'],\n",
    "            'y': row['y'],\n",
    "            'w': row['w'],\n",
    "            'h': row['h]']\n",
    "        })\n",
    "    \n",
    "    # Save to file (updated path for local C: drive)\n",
    "    output_path = f'C:/xray_data/data/{split_name}_bboxes.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(bbox_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e181c7",
   "metadata": {},
   "source": [
    "## 10. Export Metadata & Annotations\n",
    "Generate comprehensive metadata files for model training:\n",
    "- **Class mapping**: Disease ID to name mapping\n",
    "- **CSV metadata**: Train/val/test labels and patient IDs\n",
    "- **Bounding boxes**: Segmentation annotations (JSON format)\n",
    "- **Summary statistics**: Complete dataset documentation including patient-level statistics and validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# SAVE DATASET METADATA\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "# Save class mapping (updated path for local C: drive)\n",
    "class_mapping = {i: class_name for i, class_name in enumerate(CLASS_NAMES)}\n",
    "with open('C:/xray_data/data/class_mapping.json', 'w') as f:\n",
    "    json.dump(class_mapping, f, indent=2)\n",
    "\n",
    "# Save metadata for each split\n",
    "train_df.to_csv('C:/xray_data/data/train_metadata.csv', index=False)\n",
    "val_df.to_csv('C:/xray_data/data/val_metadata.csv', index=False)\n",
    "test_df.to_csv('C:/xray_data/data/test_metadata.csv', index=False)\n",
    "\n",
    "# Create comprehensive summary with patient-level and validation stats\n",
    "from collections import Counter\n",
    "\n",
    "# Extract patient IDs from each split\n",
    "train_patients = set(train_df['Image Index'].str[:8])\n",
    "val_patients = set(val_df['Image Index'].str[:8])\n",
    "test_patients = set(test_df['Image Index'].str[:8])\n",
    "\n",
    "# Count images per disease\n",
    "disease_counts = {}\n",
    "for disease in CLASS_NAMES:\n",
    "    disease_counts[disease] = {\n",
    "        'train': int(train_df[disease].sum()),\n",
    "        'val': int(val_df[disease].sum()),\n",
    "        'test': int(test_df[disease].sum()),\n",
    "        'total': int(train_df[disease].sum() + val_df[disease].sum() + test_df[disease].sum())\n",
    "    }\n",
    "\n",
    "summary = {\n",
    "    'num_classes': len(CLASS_NAMES),\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'total_images': len(sampled_df),\n",
    "    'train_images': len(train_df),\n",
    "    'val_images': len(val_df),\n",
    "    'test_images': len(test_df),\n",
    "    'images_with_bboxes_train': int(train_df['Has_BBox'].sum()),\n",
    "    'images_with_bboxes_val': int(val_df['Has_BBox'].sum()),\n",
    "    'images_with_bboxes_test': int(test_df['Has_BBox'].sum()),\n",
    "    'date_processed': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'task': 'multi-label classification (7 classes - trainable diseases only) + segmentation',\n",
    "    'multi_label': True,\n",
    "    'single_label_count': int((~sampled_df['Finding Labels'].str.contains('|', regex=False)).sum()),\n",
    "    'multi_label_count': int(sampled_df['Finding Labels'].str.contains('|', regex=False).sum()),\n",
    "    # Patient-level statistics (CRITICAL for medical AI)\n",
    "    'patient_splitting': {\n",
    "        'enabled': True,\n",
    "        'unique_patients_train': len(train_patients),\n",
    "        'unique_patients_val': len(val_patients),\n",
    "        'unique_patients_test': len(test_patients),\n",
    "        'unique_patients_total': len(train_patients) + len(val_patients) + len(test_patients),\n",
    "        'patient_overlap_train_val': len(train_patients & val_patients),\n",
    "        'patient_overlap_train_test': len(train_patients & test_patients),\n",
    "        'patient_overlap_val_test': len(val_patients & test_patients),\n",
    "        'avg_images_per_patient': len(sampled_df) / (len(train_patients) + len(val_patients) + len(test_patients))\n",
    "    },\n",
    "    # Validation results\n",
    "    'validation_status': validation_results,\n",
    "    'all_validations_passed': all(v['status'] == 'PASSED' for v in validation_results.values()),\n",
    "    # Disease distribution\n",
    "    'disease_distribution': disease_counts,\n",
    "    # Folder structure\n",
    "    'folder_structure': 'flat',  # All images directly in split folders (train/, val/, test/)\n",
    "    'labels_stored_in': 'CSV files only (multi-label compatible)',\n",
    "    'data_location': 'C:\\\\xray_data (local SSD, outside OneDrive for faster training)'\n",
    "}\n",
    "\n",
    "with open('C:/xray_data/data/dataset_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nMetadata saved to C:\\\\xray_data\\\\data\\\\\")\n",
    "print(\"  - class_mapping.json\")\n",
    "print(\"  - train_metadata.csv, val_metadata.csv, test_metadata.csv\")\n",
    "print(\"  - train_bboxes.json, val_bboxes.json, test_bboxes.json\")\n",
    "print(\"  - dataset_summary.json\")\n",
    "print(f\"\\nPatient-aware splitting:\")\n",
    "print(f\"  Train: {len(train_patients):,} patients\")\n",
    "print(f\"  Val:   {len(val_patients):,} patients\")\n",
    "print(f\"  Test:  {len(test_patients):,} patients\")\n",
    "print(f\"  No patient overlap between splits\")\n",
    "print(f\"\\nMulti-label classification (TOP 7 trainable diseases):\")\n",
    "print(f\"  Single disease: {summary['single_label_count']:,} images\")\n",
    "print(f\"  Multi-disease: {summary['multi_label_count']:,} images\")\n",
    "print(f\"  Excluded: Cardiomegaly, Consolidation, Pleural_Thickening (insufficient data)\")\n",
    "print(f\"\\nFolder structure: Flat (all images in C:\\\\xray_data\\\\data\\\\train\\\\ val\\\\ test\\\\)\")\n",
    "print(f\"Labels: Stored in CSV files (multi-label compatible)\")\n",
    "print(f\"Location: C:\\\\xray_data (local SSD, outside OneDrive)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
